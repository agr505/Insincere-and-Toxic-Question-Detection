{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "05c58dfd09ba4dd2b99f851981d6786b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_4b78af8afa77437895b2e8ffc83e1561",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_a8badc96e983447b8f1048e9db4b6c0a",
              "IPY_MODEL_5345f0e4ae0e406aa0d8ed299c0e680a"
            ]
          }
        },
        "4b78af8afa77437895b2e8ffc83e1561": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a8badc96e983447b8f1048e9db4b6c0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_0c4dacab2dc343e093e575b22d633b4d",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 213450,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 213450,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_992edb6fdcee4958b9ab4254aebd87b3"
          }
        },
        "5345f0e4ae0e406aa0d8ed299c0e680a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_dfd77804c85742699fe988c8f3f53ef6",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 213k/213k [00:00&lt;00:00, 865kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6d09ae4d274e404a924374ee1a2d5a0f"
          }
        },
        "0c4dacab2dc343e093e575b22d633b4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "992edb6fdcee4958b9ab4254aebd87b3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "dfd77804c85742699fe988c8f3f53ef6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6d09ae4d274e404a924374ee1a2d5a0f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "807c36fac76342979ab35cb7066a5042": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_0cde462cb82e4e44a55ff36be2fc1486",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_f950a3b4303a406db4f2a1e6e0f0a16b",
              "IPY_MODEL_3fecb79c1b37409fa1fece1733e5fe65"
            ]
          }
        },
        "0cde462cb82e4e44a55ff36be2fc1486": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f950a3b4303a406db4f2a1e6e0f0a16b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_23a3e6f6142d45fe83cd4c2ebdfd2531",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 361,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 361,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6d7178926b4747ab8f0ad8f0561aefa9"
          }
        },
        "3fecb79c1b37409fa1fece1733e5fe65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_a00f29770d9847f88667505eff255e19",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 361/361 [00:15&lt;00:00, 23.3B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_92e19ba5651446b38c0b7e7119e00201"
          }
        },
        "23a3e6f6142d45fe83cd4c2ebdfd2531": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6d7178926b4747ab8f0ad8f0561aefa9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a00f29770d9847f88667505eff255e19": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "92e19ba5651446b38c0b7e7119e00201": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7c408537768a4c16b5a6fc3c2a8539a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_41e23bde5d9c4b20a3667a2f33a5db9a",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_7851e29ab55043abb3b27a854a0fd6e1",
              "IPY_MODEL_bfbcae1cf4034888982b2a81245ad6fb"
            ]
          }
        },
        "41e23bde5d9c4b20a3667a2f33a5db9a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7851e29ab55043abb3b27a854a0fd6e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_fa40bf867e404521bd3e22aa93b866fb",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 435779157,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 435779157,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c130f14edb9b4bd18c7a2945854dee8e"
          }
        },
        "bfbcae1cf4034888982b2a81245ad6fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_1163cb8289234aacb5f1d3a697105781",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 436M/436M [00:12&lt;00:00, 35.5MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f686c375a10b48589d4e51a4de11f227"
          }
        },
        "fa40bf867e404521bd3e22aa93b866fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c130f14edb9b4bd18c7a2945854dee8e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1163cb8289234aacb5f1d3a697105781": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f686c375a10b48589d4e51a4de11f227": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhyHm5xDTNmP",
        "colab_type": "code",
        "outputId": "febb103f-d6a4-4110-92e7-0390bdac7d23",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XLgYtW84TJvu",
        "colab_type": "code",
        "outputId": "7e767817-94ad-4c40-9fa6-d4af2c5a4fc2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "!cp /content/drive/My\\ Drive/CSE/train_split.csv /content/train_split.csv\n",
        "!cp /content/drive/My\\ Drive/CSE/val_split.csv /content/val_split.csv"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cp: cannot stat '/content/drive/My Drive/CSE/train_split.csv': No such file or directory\n",
            "cp: cannot stat '/content/drive/My Drive/CSE/val_split.csv': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3g-6_ZeTX0q",
        "colab_type": "code",
        "outputId": "a2e98fac-7bf6-4251-84c5-c0825e3dca34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 708
        }
      },
      "source": [
        "!pip install transformers\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/78/92cedda05552398352ed9784908b834ee32a0bd071a9b32de287327370b7/transformers-2.8.0-py3-none-any.whl (563kB)\n",
            "\u001b[K     |████████████████████████████████| 573kB 4.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.38.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.2)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.12.40)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/99/50/93509f906a40bffd7d175f97fd75ea328ad9bd91f48f59c4bd084c94a25e/sacremoses-0.0.41.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 12.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 28.0MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.5.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/3f/73c881ea4723e43c1e9acf317cf407fab3a278daab3a69c98dcac511c04f/tokenizers-0.5.2-cp36-cp36m-manylinux1_x86_64.whl (3.7MB)\n",
            "\u001b[K     |████████████████████████████████| 3.7MB 39.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: botocore<1.16.0,>=1.15.40 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.15.40)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.40->boto3->transformers) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.40->boto3->transformers) (2.8.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.41-cp36-none-any.whl size=893334 sha256=73f7b685d7e60dda4d42ea6b33915a3ed094a178442546503306ba00dc61bff3\n",
            "  Stored in directory: /root/.cache/pip/wheels/22/5a/d4/b020a81249de7dc63758a34222feaa668dbe8ebfe9170cc9b1\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, sentencepiece, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.41 sentencepiece-0.1.85 tokenizers-0.5.2 transformers-2.8.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCCELooHTjB1",
        "colab_type": "code",
        "outputId": "270499b1-2499-4dfe-b654-8892e10cca3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Get the GPU device name.\n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "# The device name should look like the following:\n",
        "if device_name == '/device:GPU:0':\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "else:\n",
        "    raise SystemError('GPU device not found')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TK9gOLIATsow",
        "colab_type": "code",
        "outputId": "64a2d278-08c2-4274-e194-32d50c3def48",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla P100-PCIE-16GB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plfjfXujdQq6",
        "colab_type": "text"
      },
      "source": [
        "# Loading the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKSS2LxZUmVx",
        "colab_type": "code",
        "outputId": "303156c2-9685-48d5-bb64-93c74349e6b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        }
      },
      "source": [
        "# Read the raw dataset\n",
        "train_dataset = pd.read_csv('train_split.csv')\n",
        "train_dataset.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>qid</th>\n",
              "      <th>question_text</th>\n",
              "      <th>target</th>\n",
              "      <th>no_punctuation_lower</th>\n",
              "      <th>question_len</th>\n",
              "      <th>text_no_contraction</th>\n",
              "      <th>cleaned_text</th>\n",
              "      <th>contain_math_equation</th>\n",
              "      <th>unknow_emb_count</th>\n",
              "      <th>capitals</th>\n",
              "      <th>total_length</th>\n",
              "      <th>caps_vs_length</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>85467e55a4f58977e427</td>\n",
              "      <td>If Americans get into WW3 will Canadians be dr...</td>\n",
              "      <td>0</td>\n",
              "      <td>if americans get into ww3 will canadians be dr...</td>\n",
              "      <td>12</td>\n",
              "      <td>If Americans get into WW3 will Canadians be dr...</td>\n",
              "      <td>If Americans get into WW3 will Canadians be dr...</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>75</td>\n",
              "      <td>0.066667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>17d9f76095f6f5b903e9</td>\n",
              "      <td>How do I get started with my event management ...</td>\n",
              "      <td>0</td>\n",
              "      <td>how do i get started with my event management ...</td>\n",
              "      <td>10</td>\n",
              "      <td>How do I get started with my event management ...</td>\n",
              "      <td>How do I get started with my event management ...</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>54</td>\n",
              "      <td>0.037037</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>03865c036f1bab2cd961</td>\n",
              "      <td>Can I be a LinkedIn connection of you?</td>\n",
              "      <td>0</td>\n",
              "      <td>can i be a linkedin connection of you</td>\n",
              "      <td>8</td>\n",
              "      <td>Can I be a LinkedIn connection of you?</td>\n",
              "      <td>Can I be a LinkedIn connection of you ?</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>38</td>\n",
              "      <td>0.105263</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5b54b62de45a49f83188</td>\n",
              "      <td>Is spurting our water at someone’s face a forg...</td>\n",
              "      <td>0</td>\n",
              "      <td>is spurting our water at someones face a forgi...</td>\n",
              "      <td>15</td>\n",
              "      <td>Is spurting our water at someone’s face a forg...</td>\n",
              "      <td>Is spurting our water at someone  '  s face a ...</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>87</td>\n",
              "      <td>0.034483</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7d4841a9fcf62983e7d3</td>\n",
              "      <td>Why does the government care more for physical...</td>\n",
              "      <td>0</td>\n",
              "      <td>why does the government care more for physical...</td>\n",
              "      <td>12</td>\n",
              "      <td>Why does the government care more for physical...</td>\n",
              "      <td>Why does the government care more for physical...</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>77</td>\n",
              "      <td>0.012987</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                    qid  ... caps_vs_length\n",
              "0  85467e55a4f58977e427  ...       0.066667\n",
              "1  17d9f76095f6f5b903e9  ...       0.037037\n",
              "2  03865c036f1bab2cd961  ...       0.105263\n",
              "3  5b54b62de45a49f83188  ...       0.034483\n",
              "4  7d4841a9fcf62983e7d3  ...       0.012987\n",
              "\n",
              "[5 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQuQEKZsbPVU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get the lists of sentences and their labels.\n",
        "sentences = train_dataset.question_text.values\n",
        "labels = train_dataset.target.values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifjqPfIKeMhs",
        "colab_type": "text"
      },
      "source": [
        "# Tokenization & Input Formatting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YxgpLHNGdmq-",
        "colab_type": "code",
        "outputId": "e4fc52fb-55c0-4397-9926-743a16722b51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104,
          "referenced_widgets": [
            "05c58dfd09ba4dd2b99f851981d6786b",
            "4b78af8afa77437895b2e8ffc83e1561",
            "a8badc96e983447b8f1048e9db4b6c0a",
            "5345f0e4ae0e406aa0d8ed299c0e680a",
            "0c4dacab2dc343e093e575b22d633b4d",
            "992edb6fdcee4958b9ab4254aebd87b3",
            "dfd77804c85742699fe988c8f3f53ef6",
            "6d09ae4d274e404a924374ee1a2d5a0f"
          ]
        }
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "# Load the BERT tokenizer.\n",
        "print('Loading BERT tokenizer...')\n",
        "print('Loading BERT tokenizer...')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased', do_lower_case=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading BERT tokenizer...\n",
            "Loading BERT tokenizer...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "05c58dfd09ba4dd2b99f851981d6786b",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Downloading', max=213450, style=ProgressStyle(description_wid…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTw17rmaenlm",
        "colab_type": "code",
        "outputId": "9652762b-ae85-400e-d3fe-1cfb07185e04",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "# Print the original sentence.\n",
        "print(' Original: ', sentences[0])\n",
        "\n",
        "# Print the sentence split into tokens.\n",
        "print('Tokenized: ', tokenizer.tokenize(sentences[0]))\n",
        "\n",
        "# Print the sentence mapped to token ids.\n",
        "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[0])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Original:  If Americans get into WW3 will Canadians be drafted because they're allies?\n",
            "Tokenized:  ['If', 'Americans', 'get', 'into', 'W', '##W', '##3', 'will', 'Canadians', 'be', 'drafted', 'because', 'they', \"'\", 're', 'allies', '?']\n",
            "Token IDs:  [1409, 4038, 1243, 1154, 160, 2924, 1495, 1209, 17524, 1129, 7071, 1272, 1152, 112, 1231, 8224, 136]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qcvAzvO-ewkg",
        "colab_type": "text"
      },
      "source": [
        "## Required Formatting\n",
        "\n",
        "\n",
        "**`[SEP]`**\n",
        "\n",
        "At the end of every sentence, we need to append the special `[SEP]` token. \n",
        "\n",
        "This token is an artifact of two-sentence tasks, where BERT is given two separate sentences and asked to determine something (e.g., can the answer to the question in sentence A be found in sentence B?). \n",
        "\n",
        "I am not certain yet why the token is still required when we have only single-sentence input, but it is!\n",
        "\n",
        "**`[CLS]`**\n",
        "\n",
        "For classification tasks, we must prepend the special `[CLS]` token to the beginning of every sentence.\n",
        "\n",
        "This token has special significance. BERT consists of 12 Transformer layers. Each transformer takes in a list of token embeddings, and produces the same number of embeddings on the output (but with the feature values changed, of course!)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Ft9nlaSeqgD",
        "colab_type": "code",
        "outputId": "e23e33ac-5e05-4247-9441-c2fcc380d4ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        }
      },
      "source": [
        "max_len = 0\n",
        "\n",
        "# For every sentence...\n",
        "for sent in sentences:\n",
        "\n",
        "    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
        "    input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
        "\n",
        "    if len(input_ids) > 512:\n",
        "        print(sent)\n",
        "    # Update the maximum sentence length.\n",
        "    max_len = max(max_len, len(input_ids))\n",
        "\n",
        "print('Max sentence length: ', max_len)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (626 > 512). Running this sequence through the model will result in indexing errors\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "What is [math]\\overbrace{\\sum_{\\vartheta=8}^{\\infty} \\vec{\\frac{\\sum_{\\kappa=7}^{\\infty} \\overbrace{1x^0}^{\\text{Read carefully.}}-3x^{-1} \\div 1x^5+{\\sqrt[3]{2x^{-3}}}^{1x^0}+\\vec{\\vec{{3x^{-3}}^{1x^{-2}}}}}{\\sum_{\\dagger=9}^{\\infty} \\vec{\\boxed{\\boxed{3x^{-1}}+3x^1 \\times 1x^{-5}}}}} \\div \\sin(\\boxed{\\boxed{\\vec{3x^{-5}}}+\\sqrt[4]{2x^{-4}}+\\vec{2x^{-3}} \\div \\sin(\\sqrt[5]{\\int_{1x^5}^{2x^5} 2x^{-3} d\\varrho}) \\times \\vec{{\\underbrace{2x^1}_{\\text{Prove This.}}}^{3x^4} \\div \\sqrt[5]{2x^{-3}}+\\sum_{\\theta=8}^{\\infty} 1x^4}}) \\times {\\boxed{\\vec{\\sum_{\\nu=8}^{\\infty} \\sum_{4=6}^{\\infty} \\sum_{\\xi=9}^{\\infty} \\boxed{3x^1}-\\boxed{\\sqrt[3]{\\sqrt[3]{2x^{-2}}}}}}}^{1x^3}-\\cos({{\\tan(\\sum_{0=6}^{\\infty} \\tan(\\overbrace{\\frac{\\boxed{1x^1}-\\sqrt[3]{3x^{-2}}}{\\sum_{\\eta=10}^{\\infty} 1x^{-3} \\div 1x^1}}^{\\text{Molar Quantity.}}))}^{1x^3}}^{1x^{-4}})}^{\\text{Expanded.}}[/math]?\n",
            "Max sentence length:  628\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ufXAFsmfKVA",
        "colab_type": "code",
        "outputId": "cf99b5f9-04fe-43cc-a5dd-4afa5b70907a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 999
        }
      },
      "source": [
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "\n",
        "# For every sentence...\n",
        "for sent in sentences:\n",
        "    # `encode_plus` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    #   (5) Pad or truncate the sentence to `max_length`\n",
        "    #   (6) Create attention masks for [PAD] tokens.\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                        max_length = 512,           # Pad & truncate all sentences.\n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,   # Construct attn. masks.\n",
        "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                   )\n",
        "    \n",
        "    # Add the encoded sentence to the list.  \n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "    \n",
        "    # And its attention mask (simply differentiates padding from non-padding).\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "# Convert the lists into tensors.\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "labels = torch.tensor(labels)\n",
        "\n",
        "# Print sentence 0, now as a list of IDs.\n",
        "print('Original: ', sentences[0])\n",
        "print('Token IDs:', input_ids[0])\n",
        "print('Label:', labels[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original:  If Americans get into WW3 will Canadians be drafted because they're allies?\n",
            "Token IDs: tensor([  101,  1409,  4038,  1243,  1154,   160,  2924,  1495,  1209, 17524,\n",
            "         1129,  7071,  1272,  1152,   112,  1231,  8224,   136,   102,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0])\n",
            "Label: tensor(0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUPyUmbNggur",
        "colab_type": "text"
      },
      "source": [
        "## Train & Validation Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ylXVKz5rglL9",
        "colab_type": "code",
        "outputId": "218858e5-0249-4198-a4d0-e825e83d9d6b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from torch.utils.data import TensorDataset, random_split\n",
        "\n",
        "# Combine the training inputs into a TensorDataset.\n",
        "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
        "\n",
        "# Create a 90-10 train-validation split.\n",
        "\n",
        "# Calculate the number of samples to include in each set.\n",
        "train_size = int(0.9 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "\n",
        "# Divide the dataset by randomly selecting samples.\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "print('{:>5,} training samples'.format(train_size))\n",
        "print('{:>5,} validation samples'.format(val_size))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "881,631 training samples\n",
            "97,960 validation samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jjc-wPYLgvNV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# The DataLoader needs to know our batch size for training, so we specify it \n",
        "# here. For fine-tuning BERT on a specific task, the authors recommend a batch \n",
        "# size of 16 or 32.\n",
        "batch_size = 32\n",
        "\n",
        "# Create the DataLoaders for our training and validation sets.\n",
        "# We'll take training samples in random order. \n",
        "train_dataloader = DataLoader(\n",
        "            train_dataset,  # The training samples.\n",
        "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
        "            batch_size = batch_size # Trains with this batch size.\n",
        "        )\n",
        "\n",
        "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
        "validation_dataloader = DataLoader(\n",
        "            val_dataset, # The validation samples.\n",
        "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
        "            batch_size = batch_size # Evaluate with this batch size.\n",
        "        )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZNt0W3XGpYt",
        "colab_type": "code",
        "outputId": "08cf7057-2989-474f-905f-1b26d7f3ccb2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "drive  sample_data  train_split.csv  val_split.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qd9FTimNhgZW",
        "colab_type": "text"
      },
      "source": [
        "# Train Our Classification Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N8gpLE6ghcs3",
        "colab_type": "code",
        "outputId": "fcc370c0-388e-4846-bacd-ec369b92afac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "807c36fac76342979ab35cb7066a5042",
            "0cde462cb82e4e44a55ff36be2fc1486",
            "f950a3b4303a406db4f2a1e6e0f0a16b",
            "3fecb79c1b37409fa1fece1733e5fe65",
            "23a3e6f6142d45fe83cd4c2ebdfd2531",
            "6d7178926b4747ab8f0ad8f0561aefa9",
            "a00f29770d9847f88667505eff255e19",
            "92e19ba5651446b38c0b7e7119e00201",
            "7c408537768a4c16b5a6fc3c2a8539a7",
            "41e23bde5d9c4b20a3667a2f33a5db9a",
            "7851e29ab55043abb3b27a854a0fd6e1",
            "bfbcae1cf4034888982b2a81245ad6fb",
            "fa40bf867e404521bd3e22aa93b866fb",
            "c130f14edb9b4bd18c7a2945854dee8e",
            "1163cb8289234aacb5f1d3a697105781",
            "f686c375a10b48589d4e51a4de11f227"
          ]
        }
      },
      "source": [
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "\n",
        "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
        "# linear classification layer on top. \n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-cased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
        "    num_labels = 2, # The number of output labels--2 for binary classification.\n",
        "                    # You can increase this for multi-class tasks.   \n",
        "    output_attentions = False, # Whether the model returns attentions weights.\n",
        "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        ")\n",
        "# Load a trained model and vocabulary that you have fine-tuned\n",
        "#with open(\"/content/drive/My Drive/BERT/pytorch_model3.bin\", encoding=\"utf8\", errors='ignore') as f:  \n",
        "#with open(\"/content/drive/My Drive/BERT/pytorch_model3.bin\", 'rb') as f:\n",
        "#  text = f.read().encode('utf-8').strip()\n",
        "##############model = BertForSequenceClassification.from_pretrained(\"./model_save/\")\n",
        "#model=load_model(\"/content/drive/My Drive/BERT/\",\"pytorch_model3.bin\")\n",
        "# Copy the model to the GPU.\n",
        "model.to(device)\n",
        "# Tell pytorch to run this model on the GPU.\n",
        "model.cuda()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "807c36fac76342979ab35cb7066a5042",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Downloading', max=361, style=ProgressStyle(description_width=…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7c408537768a4c16b5a6fc3c2a8539a7",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Downloading', max=435779157, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8M9OrGLhj9F",
        "colab_type": "code",
        "outputId": "c6b3e545-68ba-464e-96b2-f0b16c5ca27f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 654
        }
      },
      "source": [
        "# Get all of the model's parameters as a list of tuples.\n",
        "params = list(model.named_parameters())\n",
        "\n",
        "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
        "\n",
        "print('==== Embedding Layer ====\\n')\n",
        "\n",
        "for p in params[0:5]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== First Transformer ====\\n')\n",
        "\n",
        "for p in params[5:21]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== Output Layer ====\\n')\n",
        "\n",
        "for p in params[-4:]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The BERT model has 201 different named parameters.\n",
            "\n",
            "==== Embedding Layer ====\n",
            "\n",
            "bert.embeddings.word_embeddings.weight                  (28996, 768)\n",
            "bert.embeddings.position_embeddings.weight                (512, 768)\n",
            "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
            "bert.embeddings.LayerNorm.weight                              (768,)\n",
            "bert.embeddings.LayerNorm.bias                                (768,)\n",
            "\n",
            "==== First Transformer ====\n",
            "\n",
            "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
            "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
            "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
            "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
            "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
            "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
            "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
            "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
            "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
            "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
            "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
            "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
            "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
            "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
            "\n",
            "==== Output Layer ====\n",
            "\n",
            "bert.pooler.dense.weight                                  (768, 768)\n",
            "bert.pooler.dense.bias                                        (768,)\n",
            "classifier.weight                                           (2, 768)\n",
            "classifier.bias                                                 (2,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8UEh2Ul4V-Qj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for param in model.parameters():\n",
        "  param.requires_grad = False\n",
        "  \n",
        "for param in model.bert.pooler.parameters():\n",
        "  param.requires_grad = True\n",
        "\n",
        "for param in model.classifier.parameters():\n",
        "  param.requires_grad = True\n",
        "  \n",
        "for param in model.bert.encoder.layer[11].parameters():\n",
        "  param.requires_grad = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SucecV7ihync",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
        "# I believe the 'W' stands for 'Weight Decay fix\"\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                )\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5VKBNnah1F5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# Number of training epochs. The BERT authors recommend between 2 and 4. \n",
        "# We chose to run for 4, but we'll see later that this may be over-fitting the\n",
        "# training data.\n",
        "epochs = 2\n",
        "\n",
        "# Total number of training steps is [number of batches] x [number of epochs]. \n",
        "# (Note that this is not the same as the number of training samples).\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                           num_training_steps = total_steps)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDTm5hakiK3G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "\n",
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "\n",
        "def get_metrics(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    tn, fp, fn, tp = metrics.confusion_matrix(labels_flat, pred_flat, labels=[0,1]).ravel()\n",
        "    return (tn, fp, fn, tp)\n",
        "\n",
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xVaSC6GClOt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_model(model, file_name):\n",
        "    torch.save(model.state_dict(), file_name)\n",
        "\n",
        "def load_model(model, file_name):\n",
        "    model.load_state_dict(torch.load(file_name))\n",
        "    return model\n",
        "model_to_save = model.module if hasattr(model, 'module') else model \n",
        "model_to_save.save_pretrained('./model_save/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CbWzxqWyiRsr",
        "colab_type": "code",
        "outputId": "02ac9cef-edaa-4c32-a701-cec6160e1100",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# This training code is based on the `run_glue.py` script here:\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "seed_val = 42\n",
        "\n",
        "\n",
        "# Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n",
        "output_dir = './model_save/'\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "# We'll store a number of quantities such as training and validation loss, \n",
        "# validation accuracy, and timings.\n",
        "training_stats = []\n",
        "\n",
        "# Measure the total training time for the whole run.\n",
        "total_t0 = time.time()\n",
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "    \n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    \n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_train_loss = 0\n",
        "\n",
        "    # Put the model into training mode. Don't be mislead--the call to \n",
        "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "    # `dropout` and `batchnorm` layers behave differently during training\n",
        "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "        # `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        # Always clear any previously calculated gradients before performing a\n",
        "        # backward pass. PyTorch doesn't do this automatically because \n",
        "        # accumulating the gradients is \"convenient while training RNNs\". \n",
        "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "        model.zero_grad()        \n",
        "\n",
        "        # Perform a forward pass (evaluate the model on this training batch).\n",
        "        # The documentation for this `model` function is here: \n",
        "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "        # It returns different numbers of parameters depending on what arguments\n",
        "        # arge given and what flags are set. For our useage here, it returns\n",
        "        # the loss (because we provided labels) and the \"logits\"--the model\n",
        "        # outputs prior to activation.\n",
        "        loss, logits = model(b_input_ids, \n",
        "                             token_type_ids=None, \n",
        "                             attention_mask=b_input_mask, \n",
        "                             labels=b_labels)\n",
        "\n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "        # single value; the `.item()` function just returns the Python value \n",
        "        # from the tensor.\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "        \n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
        "    \n",
        "    # Measure how long this epoch took.\n",
        "    training_time = format_time(time.time() - t0)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
        "        \n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "    epsilon = 1e-7\n",
        "    total_tp = 0\n",
        "    total_tn = 0\n",
        "    total_fp = 0\n",
        "    total_fn = 0\n",
        "    precision = 0\n",
        "    recall = 0\n",
        "    f1 = 0\n",
        "    nb_eval_steps = 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "        \n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using \n",
        "        # the `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "        \n",
        "        # Tell pytorch not to bother with constructing the compute graph during\n",
        "        # the forward pass, since this is only needed for backprop (training).\n",
        "        with torch.no_grad():        \n",
        "\n",
        "            # Forward pass, calculate logit predictions.\n",
        "            # token_type_ids is the same as the \"segment ids\", which \n",
        "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "            # The documentation for this `model` function is here: \n",
        "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "            # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "            # values prior to applying an activation function like the softmax.\n",
        "            (loss, logits) = model(b_input_ids, \n",
        "                                   token_type_ids=None, \n",
        "                                   attention_mask=b_input_mask,\n",
        "                                   labels=b_labels)\n",
        "            \n",
        "        # Accumulate the validation loss.\n",
        "        total_eval_loss += loss.item()\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        # Calculate the accuracy for this batch of test sentences, and\n",
        "        # accumulate it over all batches.\n",
        "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
        "        tn, fp, fn, tp = get_metrics(logits, label_ids)\n",
        "        total_tn += tn\n",
        "        total_tp += tp\n",
        "        total_fp += fp\n",
        "        total_fn += fn\n",
        "        \n",
        "\n",
        "    # Report the final accuracy, precision, recall, f1_score for this validation run.\n",
        "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
        "    print(\"  Accuracy: {0:.4f}\".format(avg_val_accuracy))\n",
        "\n",
        "    precision = total_tp / (total_tp + total_fp + epsilon)\n",
        "    print(\"  Precision: {0:.4f}\".format(precision))\n",
        "\n",
        "    recall = total_tp / (total_tp + total_fn + epsilon)\n",
        "    print(\"  Recall: {0:.4f}\".format(recall))\n",
        "\n",
        "    f1 = 2* (precision*recall) / (precision + recall + epsilon)\n",
        "    print(\"  F1 Score: {0:.4f}\".format(f1))\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
        "    \n",
        "    # Measure how long the validation run took.\n",
        "    validation_time = format_time(time.time() - t0)\n",
        "    \n",
        "    print(\"  Validation Loss: {0:.4f}\".format(avg_val_loss))\n",
        "    print(\"  Validation took: {:}\".format(validation_time))\n",
        "\n",
        "    # Record all statistics from this epoch.\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch_i + 1,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Valid. Loss': avg_val_loss,\n",
        "            'Valid. Accur.': avg_val_accuracy,\n",
        "            'Training Time': training_time,\n",
        "            'Validation Time': validation_time,\n",
        "            'Valid. Recall': recall,\n",
        "            'Valid. Precision': precision,\n",
        "            'Valid. F1': f1\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # Create output directory if needed\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    print(\"Saving model to %s\" % output_dir)\n",
        "\n",
        "    # Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
        "    # They can then be reloaded using `from_pretrained()`\n",
        "    model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
        "    model_to_save.save_pretrained(output_dir)\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")\n",
        "\n",
        "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 2 ========\n",
            "Training...\n",
            "  Batch    40  of  27,551.    Elapsed: 0:00:26.\n",
            "  Batch    80  of  27,551.    Elapsed: 0:00:51.\n",
            "  Batch   120  of  27,551.    Elapsed: 0:01:17.\n",
            "  Batch   160  of  27,551.    Elapsed: 0:01:42.\n",
            "  Batch   200  of  27,551.    Elapsed: 0:02:08.\n",
            "  Batch   240  of  27,551.    Elapsed: 0:02:33.\n",
            "  Batch   280  of  27,551.    Elapsed: 0:02:59.\n",
            "  Batch   320  of  27,551.    Elapsed: 0:03:24.\n",
            "  Batch   360  of  27,551.    Elapsed: 0:03:50.\n",
            "  Batch   400  of  27,551.    Elapsed: 0:04:15.\n",
            "  Batch   440  of  27,551.    Elapsed: 0:04:41.\n",
            "  Batch   480  of  27,551.    Elapsed: 0:05:06.\n",
            "  Batch   520  of  27,551.    Elapsed: 0:05:32.\n",
            "  Batch   560  of  27,551.    Elapsed: 0:05:57.\n",
            "  Batch   600  of  27,551.    Elapsed: 0:06:23.\n",
            "  Batch   640  of  27,551.    Elapsed: 0:06:48.\n",
            "  Batch   680  of  27,551.    Elapsed: 0:07:14.\n",
            "  Batch   720  of  27,551.    Elapsed: 0:07:39.\n",
            "  Batch   760  of  27,551.    Elapsed: 0:08:05.\n",
            "  Batch   800  of  27,551.    Elapsed: 0:08:30.\n",
            "  Batch   840  of  27,551.    Elapsed: 0:08:56.\n",
            "  Batch   880  of  27,551.    Elapsed: 0:09:21.\n",
            "  Batch   920  of  27,551.    Elapsed: 0:09:47.\n",
            "  Batch   960  of  27,551.    Elapsed: 0:10:12.\n",
            "  Batch 1,000  of  27,551.    Elapsed: 0:10:38.\n",
            "  Batch 1,040  of  27,551.    Elapsed: 0:11:03.\n",
            "  Batch 1,080  of  27,551.    Elapsed: 0:11:29.\n",
            "  Batch 1,120  of  27,551.    Elapsed: 0:11:54.\n",
            "  Batch 1,160  of  27,551.    Elapsed: 0:12:20.\n",
            "  Batch 1,200  of  27,551.    Elapsed: 0:12:45.\n",
            "  Batch 1,240  of  27,551.    Elapsed: 0:13:11.\n",
            "  Batch 1,280  of  27,551.    Elapsed: 0:13:36.\n",
            "  Batch 1,320  of  27,551.    Elapsed: 0:14:02.\n",
            "  Batch 1,360  of  27,551.    Elapsed: 0:14:27.\n",
            "  Batch 1,400  of  27,551.    Elapsed: 0:14:53.\n",
            "  Batch 1,440  of  27,551.    Elapsed: 0:15:18.\n",
            "  Batch 1,480  of  27,551.    Elapsed: 0:15:44.\n",
            "  Batch 1,520  of  27,551.    Elapsed: 0:16:10.\n",
            "  Batch 1,560  of  27,551.    Elapsed: 0:16:35.\n",
            "  Batch 1,600  of  27,551.    Elapsed: 0:17:01.\n",
            "  Batch 1,640  of  27,551.    Elapsed: 0:17:26.\n",
            "  Batch 1,680  of  27,551.    Elapsed: 0:17:52.\n",
            "  Batch 1,720  of  27,551.    Elapsed: 0:18:17.\n",
            "  Batch 1,760  of  27,551.    Elapsed: 0:18:43.\n",
            "  Batch 1,800  of  27,551.    Elapsed: 0:19:08.\n",
            "  Batch 1,840  of  27,551.    Elapsed: 0:19:34.\n",
            "  Batch 1,880  of  27,551.    Elapsed: 0:19:59.\n",
            "  Batch 1,920  of  27,551.    Elapsed: 0:20:25.\n",
            "  Batch 1,960  of  27,551.    Elapsed: 0:20:50.\n",
            "  Batch 2,000  of  27,551.    Elapsed: 0:21:16.\n",
            "  Batch 2,040  of  27,551.    Elapsed: 0:21:41.\n",
            "  Batch 2,080  of  27,551.    Elapsed: 0:22:07.\n",
            "  Batch 2,120  of  27,551.    Elapsed: 0:22:32.\n",
            "  Batch 2,160  of  27,551.    Elapsed: 0:22:58.\n",
            "  Batch 2,200  of  27,551.    Elapsed: 0:23:23.\n",
            "  Batch 2,240  of  27,551.    Elapsed: 0:23:49.\n",
            "  Batch 2,280  of  27,551.    Elapsed: 0:24:14.\n",
            "  Batch 2,320  of  27,551.    Elapsed: 0:24:40.\n",
            "  Batch 2,360  of  27,551.    Elapsed: 0:25:05.\n",
            "  Batch 2,400  of  27,551.    Elapsed: 0:25:31.\n",
            "  Batch 2,440  of  27,551.    Elapsed: 0:25:56.\n",
            "  Batch 2,480  of  27,551.    Elapsed: 0:26:22.\n",
            "  Batch 2,520  of  27,551.    Elapsed: 0:26:47.\n",
            "  Batch 2,560  of  27,551.    Elapsed: 0:27:13.\n",
            "  Batch 2,600  of  27,551.    Elapsed: 0:27:38.\n",
            "  Batch 2,640  of  27,551.    Elapsed: 0:28:04.\n",
            "  Batch 2,680  of  27,551.    Elapsed: 0:28:29.\n",
            "  Batch 2,720  of  27,551.    Elapsed: 0:28:55.\n",
            "  Batch 2,760  of  27,551.    Elapsed: 0:29:20.\n",
            "  Batch 2,800  of  27,551.    Elapsed: 0:29:46.\n",
            "  Batch 2,840  of  27,551.    Elapsed: 0:30:11.\n",
            "  Batch 2,880  of  27,551.    Elapsed: 0:30:37.\n",
            "  Batch 2,920  of  27,551.    Elapsed: 0:31:02.\n",
            "  Batch 2,960  of  27,551.    Elapsed: 0:31:28.\n",
            "  Batch 3,000  of  27,551.    Elapsed: 0:31:53.\n",
            "  Batch 3,040  of  27,551.    Elapsed: 0:32:19.\n",
            "  Batch 3,080  of  27,551.    Elapsed: 0:32:44.\n",
            "  Batch 3,120  of  27,551.    Elapsed: 0:33:10.\n",
            "  Batch 3,160  of  27,551.    Elapsed: 0:33:35.\n",
            "  Batch 3,200  of  27,551.    Elapsed: 0:34:01.\n",
            "  Batch 3,240  of  27,551.    Elapsed: 0:34:26.\n",
            "  Batch 3,280  of  27,551.    Elapsed: 0:34:52.\n",
            "  Batch 3,320  of  27,551.    Elapsed: 0:35:17.\n",
            "  Batch 3,360  of  27,551.    Elapsed: 0:35:43.\n",
            "  Batch 3,400  of  27,551.    Elapsed: 0:36:08.\n",
            "  Batch 3,440  of  27,551.    Elapsed: 0:36:34.\n",
            "  Batch 3,480  of  27,551.    Elapsed: 0:36:59.\n",
            "  Batch 3,520  of  27,551.    Elapsed: 0:37:25.\n",
            "  Batch 3,560  of  27,551.    Elapsed: 0:37:50.\n",
            "  Batch 3,600  of  27,551.    Elapsed: 0:38:16.\n",
            "  Batch 3,640  of  27,551.    Elapsed: 0:38:41.\n",
            "  Batch 3,680  of  27,551.    Elapsed: 0:39:07.\n",
            "  Batch 3,720  of  27,551.    Elapsed: 0:39:32.\n",
            "  Batch 3,760  of  27,551.    Elapsed: 0:39:58.\n",
            "  Batch 3,800  of  27,551.    Elapsed: 0:40:23.\n",
            "  Batch 3,840  of  27,551.    Elapsed: 0:40:49.\n",
            "  Batch 3,880  of  27,551.    Elapsed: 0:41:14.\n",
            "  Batch 3,920  of  27,551.    Elapsed: 0:41:40.\n",
            "  Batch 3,960  of  27,551.    Elapsed: 0:42:05.\n",
            "  Batch 4,000  of  27,551.    Elapsed: 0:42:31.\n",
            "  Batch 4,040  of  27,551.    Elapsed: 0:42:56.\n",
            "  Batch 4,080  of  27,551.    Elapsed: 0:43:22.\n",
            "  Batch 4,120  of  27,551.    Elapsed: 0:43:47.\n",
            "  Batch 4,160  of  27,551.    Elapsed: 0:44:13.\n",
            "  Batch 4,200  of  27,551.    Elapsed: 0:44:38.\n",
            "  Batch 4,240  of  27,551.    Elapsed: 0:45:04.\n",
            "  Batch 4,280  of  27,551.    Elapsed: 0:45:29.\n",
            "  Batch 4,320  of  27,551.    Elapsed: 0:45:55.\n",
            "  Batch 4,360  of  27,551.    Elapsed: 0:46:20.\n",
            "  Batch 4,400  of  27,551.    Elapsed: 0:46:46.\n",
            "  Batch 4,440  of  27,551.    Elapsed: 0:47:11.\n",
            "  Batch 4,480  of  27,551.    Elapsed: 0:47:37.\n",
            "  Batch 4,520  of  27,551.    Elapsed: 0:48:02.\n",
            "  Batch 4,560  of  27,551.    Elapsed: 0:48:28.\n",
            "  Batch 4,600  of  27,551.    Elapsed: 0:48:53.\n",
            "  Batch 4,640  of  27,551.    Elapsed: 0:49:19.\n",
            "  Batch 4,680  of  27,551.    Elapsed: 0:49:44.\n",
            "  Batch 4,720  of  27,551.    Elapsed: 0:50:10.\n",
            "  Batch 4,760  of  27,551.    Elapsed: 0:50:35.\n",
            "  Batch 4,800  of  27,551.    Elapsed: 0:51:01.\n",
            "  Batch 4,840  of  27,551.    Elapsed: 0:51:26.\n",
            "  Batch 4,880  of  27,551.    Elapsed: 0:51:52.\n",
            "  Batch 4,920  of  27,551.    Elapsed: 0:52:17.\n",
            "  Batch 4,960  of  27,551.    Elapsed: 0:52:43.\n",
            "  Batch 5,000  of  27,551.    Elapsed: 0:53:08.\n",
            "  Batch 5,040  of  27,551.    Elapsed: 0:53:34.\n",
            "  Batch 5,080  of  27,551.    Elapsed: 0:53:59.\n",
            "  Batch 5,120  of  27,551.    Elapsed: 0:54:25.\n",
            "  Batch 5,160  of  27,551.    Elapsed: 0:54:50.\n",
            "  Batch 5,200  of  27,551.    Elapsed: 0:55:16.\n",
            "  Batch 5,240  of  27,551.    Elapsed: 0:55:41.\n",
            "  Batch 5,280  of  27,551.    Elapsed: 0:56:07.\n",
            "  Batch 5,320  of  27,551.    Elapsed: 0:56:32.\n",
            "  Batch 5,360  of  27,551.    Elapsed: 0:56:58.\n",
            "  Batch 5,400  of  27,551.    Elapsed: 0:57:23.\n",
            "  Batch 5,440  of  27,551.    Elapsed: 0:57:49.\n",
            "  Batch 5,480  of  27,551.    Elapsed: 0:58:14.\n",
            "  Batch 5,520  of  27,551.    Elapsed: 0:58:40.\n",
            "  Batch 5,560  of  27,551.    Elapsed: 0:59:05.\n",
            "  Batch 5,600  of  27,551.    Elapsed: 0:59:31.\n",
            "  Batch 5,640  of  27,551.    Elapsed: 0:59:56.\n",
            "  Batch 5,680  of  27,551.    Elapsed: 1:00:22.\n",
            "  Batch 5,720  of  27,551.    Elapsed: 1:00:47.\n",
            "  Batch 5,760  of  27,551.    Elapsed: 1:01:13.\n",
            "  Batch 5,800  of  27,551.    Elapsed: 1:01:38.\n",
            "  Batch 5,840  of  27,551.    Elapsed: 1:02:04.\n",
            "  Batch 5,880  of  27,551.    Elapsed: 1:02:29.\n",
            "  Batch 5,920  of  27,551.    Elapsed: 1:02:55.\n",
            "  Batch 5,960  of  27,551.    Elapsed: 1:03:20.\n",
            "  Batch 6,000  of  27,551.    Elapsed: 1:03:46.\n",
            "  Batch 6,040  of  27,551.    Elapsed: 1:04:11.\n",
            "  Batch 6,080  of  27,551.    Elapsed: 1:04:37.\n",
            "  Batch 6,120  of  27,551.    Elapsed: 1:05:02.\n",
            "  Batch 6,160  of  27,551.    Elapsed: 1:05:28.\n",
            "  Batch 6,200  of  27,551.    Elapsed: 1:05:53.\n",
            "  Batch 6,240  of  27,551.    Elapsed: 1:06:19.\n",
            "  Batch 6,280  of  27,551.    Elapsed: 1:06:44.\n",
            "  Batch 6,320  of  27,551.    Elapsed: 1:07:10.\n",
            "  Batch 6,360  of  27,551.    Elapsed: 1:07:35.\n",
            "  Batch 6,400  of  27,551.    Elapsed: 1:08:01.\n",
            "  Batch 6,440  of  27,551.    Elapsed: 1:08:26.\n",
            "  Batch 6,480  of  27,551.    Elapsed: 1:08:52.\n",
            "  Batch 6,520  of  27,551.    Elapsed: 1:09:17.\n",
            "  Batch 6,560  of  27,551.    Elapsed: 1:09:43.\n",
            "  Batch 6,600  of  27,551.    Elapsed: 1:10:08.\n",
            "  Batch 6,640  of  27,551.    Elapsed: 1:10:34.\n",
            "  Batch 6,680  of  27,551.    Elapsed: 1:10:59.\n",
            "  Batch 6,720  of  27,551.    Elapsed: 1:11:25.\n",
            "  Batch 6,760  of  27,551.    Elapsed: 1:11:50.\n",
            "  Batch 6,800  of  27,551.    Elapsed: 1:12:16.\n",
            "  Batch 6,840  of  27,551.    Elapsed: 1:12:41.\n",
            "  Batch 6,880  of  27,551.    Elapsed: 1:13:07.\n",
            "  Batch 6,920  of  27,551.    Elapsed: 1:13:32.\n",
            "  Batch 6,960  of  27,551.    Elapsed: 1:13:58.\n",
            "  Batch 7,000  of  27,551.    Elapsed: 1:14:23.\n",
            "  Batch 7,040  of  27,551.    Elapsed: 1:14:49.\n",
            "  Batch 7,080  of  27,551.    Elapsed: 1:15:14.\n",
            "  Batch 7,120  of  27,551.    Elapsed: 1:15:40.\n",
            "  Batch 7,160  of  27,551.    Elapsed: 1:16:05.\n",
            "  Batch 7,200  of  27,551.    Elapsed: 1:16:31.\n",
            "  Batch 7,240  of  27,551.    Elapsed: 1:16:56.\n",
            "  Batch 7,280  of  27,551.    Elapsed: 1:17:22.\n",
            "  Batch 7,320  of  27,551.    Elapsed: 1:17:47.\n",
            "  Batch 7,360  of  27,551.    Elapsed: 1:18:12.\n",
            "  Batch 7,400  of  27,551.    Elapsed: 1:18:38.\n",
            "  Batch 7,440  of  27,551.    Elapsed: 1:19:03.\n",
            "  Batch 7,480  of  27,551.    Elapsed: 1:19:29.\n",
            "  Batch 7,520  of  27,551.    Elapsed: 1:19:54.\n",
            "  Batch 7,560  of  27,551.    Elapsed: 1:20:20.\n",
            "  Batch 7,600  of  27,551.    Elapsed: 1:20:45.\n",
            "  Batch 7,640  of  27,551.    Elapsed: 1:21:11.\n",
            "  Batch 7,680  of  27,551.    Elapsed: 1:21:36.\n",
            "  Batch 7,720  of  27,551.    Elapsed: 1:22:02.\n",
            "  Batch 7,760  of  27,551.    Elapsed: 1:22:27.\n",
            "  Batch 7,800  of  27,551.    Elapsed: 1:22:53.\n",
            "  Batch 7,840  of  27,551.    Elapsed: 1:23:18.\n",
            "  Batch 7,880  of  27,551.    Elapsed: 1:23:44.\n",
            "  Batch 7,920  of  27,551.    Elapsed: 1:24:09.\n",
            "  Batch 7,960  of  27,551.    Elapsed: 1:24:35.\n",
            "  Batch 8,000  of  27,551.    Elapsed: 1:25:00.\n",
            "  Batch 8,040  of  27,551.    Elapsed: 1:25:26.\n",
            "  Batch 8,080  of  27,551.    Elapsed: 1:25:51.\n",
            "  Batch 8,120  of  27,551.    Elapsed: 1:26:17.\n",
            "  Batch 8,160  of  27,551.    Elapsed: 1:26:42.\n",
            "  Batch 8,200  of  27,551.    Elapsed: 1:27:08.\n",
            "  Batch 8,240  of  27,551.    Elapsed: 1:27:33.\n",
            "  Batch 8,280  of  27,551.    Elapsed: 1:27:59.\n",
            "  Batch 8,320  of  27,551.    Elapsed: 1:28:24.\n",
            "  Batch 8,360  of  27,551.    Elapsed: 1:28:50.\n",
            "  Batch 8,400  of  27,551.    Elapsed: 1:29:15.\n",
            "  Batch 8,440  of  27,551.    Elapsed: 1:29:41.\n",
            "  Batch 8,480  of  27,551.    Elapsed: 1:30:06.\n",
            "  Batch 8,520  of  27,551.    Elapsed: 1:30:32.\n",
            "  Batch 8,560  of  27,551.    Elapsed: 1:30:57.\n",
            "  Batch 8,600  of  27,551.    Elapsed: 1:31:23.\n",
            "  Batch 8,640  of  27,551.    Elapsed: 1:31:48.\n",
            "  Batch 8,680  of  27,551.    Elapsed: 1:32:14.\n",
            "  Batch 8,720  of  27,551.    Elapsed: 1:32:39.\n",
            "  Batch 8,760  of  27,551.    Elapsed: 1:33:05.\n",
            "  Batch 8,800  of  27,551.    Elapsed: 1:33:30.\n",
            "  Batch 8,840  of  27,551.    Elapsed: 1:33:56.\n",
            "  Batch 8,880  of  27,551.    Elapsed: 1:34:21.\n",
            "  Batch 8,920  of  27,551.    Elapsed: 1:34:47.\n",
            "  Batch 8,960  of  27,551.    Elapsed: 1:35:12.\n",
            "  Batch 9,000  of  27,551.    Elapsed: 1:35:38.\n",
            "  Batch 9,040  of  27,551.    Elapsed: 1:36:03.\n",
            "  Batch 9,080  of  27,551.    Elapsed: 1:36:28.\n",
            "  Batch 9,120  of  27,551.    Elapsed: 1:36:54.\n",
            "  Batch 9,160  of  27,551.    Elapsed: 1:37:19.\n",
            "  Batch 9,200  of  27,551.    Elapsed: 1:37:45.\n",
            "  Batch 9,240  of  27,551.    Elapsed: 1:38:10.\n",
            "  Batch 9,280  of  27,551.    Elapsed: 1:38:36.\n",
            "  Batch 9,320  of  27,551.    Elapsed: 1:39:01.\n",
            "  Batch 9,360  of  27,551.    Elapsed: 1:39:27.\n",
            "  Batch 9,400  of  27,551.    Elapsed: 1:39:52.\n",
            "  Batch 9,440  of  27,551.    Elapsed: 1:40:18.\n",
            "  Batch 9,480  of  27,551.    Elapsed: 1:40:43.\n",
            "  Batch 9,520  of  27,551.    Elapsed: 1:41:09.\n",
            "  Batch 9,560  of  27,551.    Elapsed: 1:41:34.\n",
            "  Batch 9,600  of  27,551.    Elapsed: 1:42:00.\n",
            "  Batch 9,640  of  27,551.    Elapsed: 1:42:25.\n",
            "  Batch 9,680  of  27,551.    Elapsed: 1:42:51.\n",
            "  Batch 9,720  of  27,551.    Elapsed: 1:43:16.\n",
            "  Batch 9,760  of  27,551.    Elapsed: 1:43:42.\n",
            "  Batch 9,800  of  27,551.    Elapsed: 1:44:07.\n",
            "  Batch 9,840  of  27,551.    Elapsed: 1:44:33.\n",
            "  Batch 9,880  of  27,551.    Elapsed: 1:44:58.\n",
            "  Batch 9,920  of  27,551.    Elapsed: 1:45:24.\n",
            "  Batch 9,960  of  27,551.    Elapsed: 1:45:49.\n",
            "  Batch 10,000  of  27,551.    Elapsed: 1:46:15.\n",
            "  Batch 10,040  of  27,551.    Elapsed: 1:46:40.\n",
            "  Batch 10,080  of  27,551.    Elapsed: 1:47:06.\n",
            "  Batch 10,120  of  27,551.    Elapsed: 1:47:31.\n",
            "  Batch 10,160  of  27,551.    Elapsed: 1:47:57.\n",
            "  Batch 10,200  of  27,551.    Elapsed: 1:48:22.\n",
            "  Batch 10,240  of  27,551.    Elapsed: 1:48:48.\n",
            "  Batch 10,280  of  27,551.    Elapsed: 1:49:13.\n",
            "  Batch 10,320  of  27,551.    Elapsed: 1:49:39.\n",
            "  Batch 10,360  of  27,551.    Elapsed: 1:50:04.\n",
            "  Batch 10,400  of  27,551.    Elapsed: 1:50:30.\n",
            "  Batch 10,440  of  27,551.    Elapsed: 1:50:55.\n",
            "  Batch 10,480  of  27,551.    Elapsed: 1:51:21.\n",
            "  Batch 10,520  of  27,551.    Elapsed: 1:51:46.\n",
            "  Batch 10,560  of  27,551.    Elapsed: 1:52:12.\n",
            "  Batch 10,600  of  27,551.    Elapsed: 1:52:37.\n",
            "  Batch 10,640  of  27,551.    Elapsed: 1:53:03.\n",
            "  Batch 10,680  of  27,551.    Elapsed: 1:53:28.\n",
            "  Batch 10,720  of  27,551.    Elapsed: 1:53:54.\n",
            "  Batch 10,760  of  27,551.    Elapsed: 1:54:19.\n",
            "  Batch 10,800  of  27,551.    Elapsed: 1:54:45.\n",
            "  Batch 10,840  of  27,551.    Elapsed: 1:55:10.\n",
            "  Batch 10,880  of  27,551.    Elapsed: 1:55:36.\n",
            "  Batch 10,920  of  27,551.    Elapsed: 1:56:01.\n",
            "  Batch 10,960  of  27,551.    Elapsed: 1:56:27.\n",
            "  Batch 11,000  of  27,551.    Elapsed: 1:56:52.\n",
            "  Batch 11,040  of  27,551.    Elapsed: 1:57:18.\n",
            "  Batch 11,080  of  27,551.    Elapsed: 1:57:43.\n",
            "  Batch 11,120  of  27,551.    Elapsed: 1:58:09.\n",
            "  Batch 11,160  of  27,551.    Elapsed: 1:58:34.\n",
            "  Batch 11,200  of  27,551.    Elapsed: 1:59:00.\n",
            "  Batch 11,240  of  27,551.    Elapsed: 1:59:25.\n",
            "  Batch 11,280  of  27,551.    Elapsed: 1:59:51.\n",
            "  Batch 11,320  of  27,551.    Elapsed: 2:00:16.\n",
            "  Batch 11,360  of  27,551.    Elapsed: 2:00:42.\n",
            "  Batch 11,400  of  27,551.    Elapsed: 2:01:07.\n",
            "  Batch 11,440  of  27,551.    Elapsed: 2:01:33.\n",
            "  Batch 11,480  of  27,551.    Elapsed: 2:01:58.\n",
            "  Batch 11,520  of  27,551.    Elapsed: 2:02:24.\n",
            "  Batch 11,560  of  27,551.    Elapsed: 2:02:49.\n",
            "  Batch 11,600  of  27,551.    Elapsed: 2:03:15.\n",
            "  Batch 11,640  of  27,551.    Elapsed: 2:03:40.\n",
            "  Batch 11,680  of  27,551.    Elapsed: 2:04:05.\n",
            "  Batch 11,720  of  27,551.    Elapsed: 2:04:31.\n",
            "  Batch 11,760  of  27,551.    Elapsed: 2:04:56.\n",
            "  Batch 11,800  of  27,551.    Elapsed: 2:05:22.\n",
            "  Batch 11,840  of  27,551.    Elapsed: 2:05:47.\n",
            "  Batch 11,880  of  27,551.    Elapsed: 2:06:13.\n",
            "  Batch 11,920  of  27,551.    Elapsed: 2:06:38.\n",
            "  Batch 11,960  of  27,551.    Elapsed: 2:07:04.\n",
            "  Batch 12,000  of  27,551.    Elapsed: 2:07:29.\n",
            "  Batch 12,040  of  27,551.    Elapsed: 2:07:55.\n",
            "  Batch 12,080  of  27,551.    Elapsed: 2:08:20.\n",
            "  Batch 12,120  of  27,551.    Elapsed: 2:08:46.\n",
            "  Batch 12,160  of  27,551.    Elapsed: 2:09:11.\n",
            "  Batch 12,200  of  27,551.    Elapsed: 2:09:37.\n",
            "  Batch 12,240  of  27,551.    Elapsed: 2:10:02.\n",
            "  Batch 12,280  of  27,551.    Elapsed: 2:10:28.\n",
            "  Batch 12,320  of  27,551.    Elapsed: 2:10:53.\n",
            "  Batch 12,360  of  27,551.    Elapsed: 2:11:19.\n",
            "  Batch 12,400  of  27,551.    Elapsed: 2:11:44.\n",
            "  Batch 12,440  of  27,551.    Elapsed: 2:12:10.\n",
            "  Batch 12,480  of  27,551.    Elapsed: 2:12:35.\n",
            "  Batch 12,520  of  27,551.    Elapsed: 2:13:01.\n",
            "  Batch 12,560  of  27,551.    Elapsed: 2:13:26.\n",
            "  Batch 12,600  of  27,551.    Elapsed: 2:13:52.\n",
            "  Batch 12,640  of  27,551.    Elapsed: 2:14:17.\n",
            "  Batch 12,680  of  27,551.    Elapsed: 2:14:43.\n",
            "  Batch 12,720  of  27,551.    Elapsed: 2:15:08.\n",
            "  Batch 12,760  of  27,551.    Elapsed: 2:15:34.\n",
            "  Batch 12,800  of  27,551.    Elapsed: 2:15:59.\n",
            "  Batch 12,840  of  27,551.    Elapsed: 2:16:25.\n",
            "  Batch 12,880  of  27,551.    Elapsed: 2:16:50.\n",
            "  Batch 12,920  of  27,551.    Elapsed: 2:17:16.\n",
            "  Batch 12,960  of  27,551.    Elapsed: 2:17:41.\n",
            "  Batch 13,000  of  27,551.    Elapsed: 2:18:07.\n",
            "  Batch 13,040  of  27,551.    Elapsed: 2:18:32.\n",
            "  Batch 13,080  of  27,551.    Elapsed: 2:18:58.\n",
            "  Batch 13,120  of  27,551.    Elapsed: 2:19:23.\n",
            "  Batch 13,160  of  27,551.    Elapsed: 2:19:49.\n",
            "  Batch 13,200  of  27,551.    Elapsed: 2:20:14.\n",
            "  Batch 13,240  of  27,551.    Elapsed: 2:20:40.\n",
            "  Batch 13,280  of  27,551.    Elapsed: 2:21:05.\n",
            "  Batch 13,320  of  27,551.    Elapsed: 2:21:31.\n",
            "  Batch 13,360  of  27,551.    Elapsed: 2:21:56.\n",
            "  Batch 13,400  of  27,551.    Elapsed: 2:22:22.\n",
            "  Batch 13,440  of  27,551.    Elapsed: 2:22:47.\n",
            "  Batch 13,480  of  27,551.    Elapsed: 2:23:13.\n",
            "  Batch 13,520  of  27,551.    Elapsed: 2:23:38.\n",
            "  Batch 13,560  of  27,551.    Elapsed: 2:24:04.\n",
            "  Batch 13,600  of  27,551.    Elapsed: 2:24:29.\n",
            "  Batch 13,640  of  27,551.    Elapsed: 2:24:55.\n",
            "  Batch 13,680  of  27,551.    Elapsed: 2:25:20.\n",
            "  Batch 13,720  of  27,551.    Elapsed: 2:25:46.\n",
            "  Batch 13,760  of  27,551.    Elapsed: 2:26:11.\n",
            "  Batch 13,800  of  27,551.    Elapsed: 2:26:37.\n",
            "  Batch 13,840  of  27,551.    Elapsed: 2:27:02.\n",
            "  Batch 13,880  of  27,551.    Elapsed: 2:27:28.\n",
            "  Batch 13,920  of  27,551.    Elapsed: 2:27:53.\n",
            "  Batch 13,960  of  27,551.    Elapsed: 2:28:19.\n",
            "  Batch 14,000  of  27,551.    Elapsed: 2:28:44.\n",
            "  Batch 14,040  of  27,551.    Elapsed: 2:29:10.\n",
            "  Batch 14,080  of  27,551.    Elapsed: 2:29:35.\n",
            "  Batch 14,120  of  27,551.    Elapsed: 2:30:01.\n",
            "  Batch 14,160  of  27,551.    Elapsed: 2:30:26.\n",
            "  Batch 14,200  of  27,551.    Elapsed: 2:30:52.\n",
            "  Batch 14,240  of  27,551.    Elapsed: 2:31:17.\n",
            "  Batch 14,280  of  27,551.    Elapsed: 2:31:43.\n",
            "  Batch 14,320  of  27,551.    Elapsed: 2:32:08.\n",
            "  Batch 14,360  of  27,551.    Elapsed: 2:32:34.\n",
            "  Batch 14,400  of  27,551.    Elapsed: 2:32:59.\n",
            "  Batch 14,440  of  27,551.    Elapsed: 2:33:25.\n",
            "  Batch 14,480  of  27,551.    Elapsed: 2:33:50.\n",
            "  Batch 14,520  of  27,551.    Elapsed: 2:34:16.\n",
            "  Batch 14,560  of  27,551.    Elapsed: 2:34:41.\n",
            "  Batch 14,600  of  27,551.    Elapsed: 2:35:07.\n",
            "  Batch 14,640  of  27,551.    Elapsed: 2:35:32.\n",
            "  Batch 14,680  of  27,551.    Elapsed: 2:35:58.\n",
            "  Batch 14,720  of  27,551.    Elapsed: 2:36:23.\n",
            "  Batch 14,760  of  27,551.    Elapsed: 2:36:49.\n",
            "  Batch 14,800  of  27,551.    Elapsed: 2:37:14.\n",
            "  Batch 14,840  of  27,551.    Elapsed: 2:37:40.\n",
            "  Batch 14,880  of  27,551.    Elapsed: 2:38:05.\n",
            "  Batch 14,920  of  27,551.    Elapsed: 2:38:31.\n",
            "  Batch 14,960  of  27,551.    Elapsed: 2:38:56.\n",
            "  Batch 15,000  of  27,551.    Elapsed: 2:39:22.\n",
            "  Batch 15,040  of  27,551.    Elapsed: 2:39:47.\n",
            "  Batch 15,080  of  27,551.    Elapsed: 2:40:13.\n",
            "  Batch 15,120  of  27,551.    Elapsed: 2:40:38.\n",
            "  Batch 15,160  of  27,551.    Elapsed: 2:41:04.\n",
            "  Batch 15,200  of  27,551.    Elapsed: 2:41:29.\n",
            "  Batch 15,240  of  27,551.    Elapsed: 2:41:54.\n",
            "  Batch 15,280  of  27,551.    Elapsed: 2:42:20.\n",
            "  Batch 15,320  of  27,551.    Elapsed: 2:42:45.\n",
            "  Batch 15,360  of  27,551.    Elapsed: 2:43:11.\n",
            "  Batch 15,400  of  27,551.    Elapsed: 2:43:36.\n",
            "  Batch 15,440  of  27,551.    Elapsed: 2:44:02.\n",
            "  Batch 15,480  of  27,551.    Elapsed: 2:44:27.\n",
            "  Batch 15,520  of  27,551.    Elapsed: 2:44:53.\n",
            "  Batch 15,560  of  27,551.    Elapsed: 2:45:18.\n",
            "  Batch 15,600  of  27,551.    Elapsed: 2:45:44.\n",
            "  Batch 15,640  of  27,551.    Elapsed: 2:46:09.\n",
            "  Batch 15,680  of  27,551.    Elapsed: 2:46:35.\n",
            "  Batch 15,720  of  27,551.    Elapsed: 2:47:00.\n",
            "  Batch 15,760  of  27,551.    Elapsed: 2:47:26.\n",
            "  Batch 15,800  of  27,551.    Elapsed: 2:47:51.\n",
            "  Batch 15,840  of  27,551.    Elapsed: 2:48:17.\n",
            "  Batch 15,880  of  27,551.    Elapsed: 2:48:42.\n",
            "  Batch 15,920  of  27,551.    Elapsed: 2:49:08.\n",
            "  Batch 15,960  of  27,551.    Elapsed: 2:49:33.\n",
            "  Batch 16,000  of  27,551.    Elapsed: 2:49:59.\n",
            "  Batch 16,040  of  27,551.    Elapsed: 2:50:24.\n",
            "  Batch 16,080  of  27,551.    Elapsed: 2:50:50.\n",
            "  Batch 16,120  of  27,551.    Elapsed: 2:51:15.\n",
            "  Batch 16,160  of  27,551.    Elapsed: 2:51:41.\n",
            "  Batch 16,200  of  27,551.    Elapsed: 2:52:06.\n",
            "  Batch 16,240  of  27,551.    Elapsed: 2:52:32.\n",
            "  Batch 16,280  of  27,551.    Elapsed: 2:52:57.\n",
            "  Batch 16,320  of  27,551.    Elapsed: 2:53:23.\n",
            "  Batch 16,360  of  27,551.    Elapsed: 2:53:48.\n",
            "  Batch 16,400  of  27,551.    Elapsed: 2:54:14.\n",
            "  Batch 16,440  of  27,551.    Elapsed: 2:54:39.\n",
            "  Batch 16,480  of  27,551.    Elapsed: 2:55:05.\n",
            "  Batch 16,520  of  27,551.    Elapsed: 2:55:30.\n",
            "  Batch 16,560  of  27,551.    Elapsed: 2:55:56.\n",
            "  Batch 16,600  of  27,551.    Elapsed: 2:56:21.\n",
            "  Batch 16,640  of  27,551.    Elapsed: 2:56:47.\n",
            "  Batch 16,680  of  27,551.    Elapsed: 2:57:12.\n",
            "  Batch 16,720  of  27,551.    Elapsed: 2:57:38.\n",
            "  Batch 16,760  of  27,551.    Elapsed: 2:58:03.\n",
            "  Batch 16,800  of  27,551.    Elapsed: 2:58:29.\n",
            "  Batch 16,840  of  27,551.    Elapsed: 2:58:54.\n",
            "  Batch 16,880  of  27,551.    Elapsed: 2:59:20.\n",
            "  Batch 16,920  of  27,551.    Elapsed: 2:59:45.\n",
            "  Batch 16,960  of  27,551.    Elapsed: 3:00:11.\n",
            "  Batch 17,000  of  27,551.    Elapsed: 3:00:36.\n",
            "  Batch 17,040  of  27,551.    Elapsed: 3:01:02.\n",
            "  Batch 17,080  of  27,551.    Elapsed: 3:01:27.\n",
            "  Batch 17,120  of  27,551.    Elapsed: 3:01:53.\n",
            "  Batch 17,160  of  27,551.    Elapsed: 3:02:18.\n",
            "  Batch 17,200  of  27,551.    Elapsed: 3:02:44.\n",
            "  Batch 17,240  of  27,551.    Elapsed: 3:03:09.\n",
            "  Batch 17,280  of  27,551.    Elapsed: 3:03:35.\n",
            "  Batch 17,320  of  27,551.    Elapsed: 3:04:00.\n",
            "  Batch 17,360  of  27,551.    Elapsed: 3:04:26.\n",
            "  Batch 17,400  of  27,551.    Elapsed: 3:04:51.\n",
            "  Batch 17,440  of  27,551.    Elapsed: 3:05:17.\n",
            "  Batch 17,480  of  27,551.    Elapsed: 3:05:42.\n",
            "  Batch 17,520  of  27,551.    Elapsed: 3:06:08.\n",
            "  Batch 17,560  of  27,551.    Elapsed: 3:06:33.\n",
            "  Batch 17,600  of  27,551.    Elapsed: 3:06:59.\n",
            "  Batch 17,640  of  27,551.    Elapsed: 3:07:24.\n",
            "  Batch 17,680  of  27,551.    Elapsed: 3:07:49.\n",
            "  Batch 17,720  of  27,551.    Elapsed: 3:08:15.\n",
            "  Batch 17,760  of  27,551.    Elapsed: 3:08:40.\n",
            "  Batch 17,800  of  27,551.    Elapsed: 3:09:06.\n",
            "  Batch 17,840  of  27,551.    Elapsed: 3:09:31.\n",
            "  Batch 17,880  of  27,551.    Elapsed: 3:09:57.\n",
            "  Batch 17,920  of  27,551.    Elapsed: 3:10:22.\n",
            "  Batch 17,960  of  27,551.    Elapsed: 3:10:48.\n",
            "  Batch 18,000  of  27,551.    Elapsed: 3:11:13.\n",
            "  Batch 18,040  of  27,551.    Elapsed: 3:11:39.\n",
            "  Batch 18,080  of  27,551.    Elapsed: 3:12:04.\n",
            "  Batch 18,120  of  27,551.    Elapsed: 3:12:30.\n",
            "  Batch 18,160  of  27,551.    Elapsed: 3:12:55.\n",
            "  Batch 18,200  of  27,551.    Elapsed: 3:13:21.\n",
            "  Batch 18,240  of  27,551.    Elapsed: 3:13:46.\n",
            "  Batch 18,280  of  27,551.    Elapsed: 3:14:12.\n",
            "  Batch 18,320  of  27,551.    Elapsed: 3:14:37.\n",
            "  Batch 18,360  of  27,551.    Elapsed: 3:15:03.\n",
            "  Batch 18,400  of  27,551.    Elapsed: 3:15:28.\n",
            "  Batch 18,440  of  27,551.    Elapsed: 3:15:54.\n",
            "  Batch 18,480  of  27,551.    Elapsed: 3:16:19.\n",
            "  Batch 18,520  of  27,551.    Elapsed: 3:16:45.\n",
            "  Batch 18,560  of  27,551.    Elapsed: 3:17:10.\n",
            "  Batch 18,600  of  27,551.    Elapsed: 3:17:36.\n",
            "  Batch 18,640  of  27,551.    Elapsed: 3:18:01.\n",
            "  Batch 18,680  of  27,551.    Elapsed: 3:18:27.\n",
            "  Batch 18,720  of  27,551.    Elapsed: 3:18:52.\n",
            "  Batch 18,760  of  27,551.    Elapsed: 3:19:18.\n",
            "  Batch 18,800  of  27,551.    Elapsed: 3:19:43.\n",
            "  Batch 18,840  of  27,551.    Elapsed: 3:20:09.\n",
            "  Batch 18,880  of  27,551.    Elapsed: 3:20:34.\n",
            "  Batch 18,920  of  27,551.    Elapsed: 3:21:00.\n",
            "  Batch 18,960  of  27,551.    Elapsed: 3:21:25.\n",
            "  Batch 19,000  of  27,551.    Elapsed: 3:21:51.\n",
            "  Batch 19,040  of  27,551.    Elapsed: 3:22:16.\n",
            "  Batch 19,080  of  27,551.    Elapsed: 3:22:42.\n",
            "  Batch 19,120  of  27,551.    Elapsed: 3:23:07.\n",
            "  Batch 19,160  of  27,551.    Elapsed: 3:23:33.\n",
            "  Batch 19,200  of  27,551.    Elapsed: 3:23:58.\n",
            "  Batch 19,240  of  27,551.    Elapsed: 3:24:24.\n",
            "  Batch 19,280  of  27,551.    Elapsed: 3:24:49.\n",
            "  Batch 19,320  of  27,551.    Elapsed: 3:25:15.\n",
            "  Batch 19,360  of  27,551.    Elapsed: 3:25:40.\n",
            "  Batch 19,400  of  27,551.    Elapsed: 3:26:06.\n",
            "  Batch 19,440  of  27,551.    Elapsed: 3:26:31.\n",
            "  Batch 19,480  of  27,551.    Elapsed: 3:26:57.\n",
            "  Batch 19,520  of  27,551.    Elapsed: 3:27:22.\n",
            "  Batch 19,560  of  27,551.    Elapsed: 3:27:48.\n",
            "  Batch 19,600  of  27,551.    Elapsed: 3:28:13.\n",
            "  Batch 19,640  of  27,551.    Elapsed: 3:28:39.\n",
            "  Batch 19,680  of  27,551.    Elapsed: 3:29:04.\n",
            "  Batch 19,720  of  27,551.    Elapsed: 3:29:30.\n",
            "  Batch 19,760  of  27,551.    Elapsed: 3:29:55.\n",
            "  Batch 19,800  of  27,551.    Elapsed: 3:30:21.\n",
            "  Batch 19,840  of  27,551.    Elapsed: 3:30:46.\n",
            "  Batch 19,880  of  27,551.    Elapsed: 3:31:12.\n",
            "  Batch 19,920  of  27,551.    Elapsed: 3:31:37.\n",
            "  Batch 19,960  of  27,551.    Elapsed: 3:32:03.\n",
            "  Batch 20,000  of  27,551.    Elapsed: 3:32:28.\n",
            "  Batch 20,040  of  27,551.    Elapsed: 3:32:54.\n",
            "  Batch 20,080  of  27,551.    Elapsed: 3:33:19.\n",
            "  Batch 20,120  of  27,551.    Elapsed: 3:33:45.\n",
            "  Batch 20,160  of  27,551.    Elapsed: 3:34:10.\n",
            "  Batch 20,200  of  27,551.    Elapsed: 3:34:36.\n",
            "  Batch 20,240  of  27,551.    Elapsed: 3:35:01.\n",
            "  Batch 20,280  of  27,551.    Elapsed: 3:35:27.\n",
            "  Batch 20,320  of  27,551.    Elapsed: 3:35:52.\n",
            "  Batch 20,360  of  27,551.    Elapsed: 3:36:18.\n",
            "  Batch 20,400  of  27,551.    Elapsed: 3:36:43.\n",
            "  Batch 20,440  of  27,551.    Elapsed: 3:37:09.\n",
            "  Batch 20,480  of  27,551.    Elapsed: 3:37:34.\n",
            "  Batch 20,520  of  27,551.    Elapsed: 3:38:00.\n",
            "  Batch 20,560  of  27,551.    Elapsed: 3:38:25.\n",
            "  Batch 20,600  of  27,551.    Elapsed: 3:38:51.\n",
            "  Batch 20,640  of  27,551.    Elapsed: 3:39:16.\n",
            "  Batch 20,680  of  27,551.    Elapsed: 3:39:42.\n",
            "  Batch 20,720  of  27,551.    Elapsed: 3:40:07.\n",
            "  Batch 20,760  of  27,551.    Elapsed: 3:40:33.\n",
            "  Batch 20,800  of  27,551.    Elapsed: 3:40:58.\n",
            "  Batch 20,840  of  27,551.    Elapsed: 3:41:24.\n",
            "  Batch 20,880  of  27,551.    Elapsed: 3:41:49.\n",
            "  Batch 20,920  of  27,551.    Elapsed: 3:42:15.\n",
            "  Batch 20,960  of  27,551.    Elapsed: 3:42:40.\n",
            "  Batch 21,000  of  27,551.    Elapsed: 3:43:06.\n",
            "  Batch 21,040  of  27,551.    Elapsed: 3:43:31.\n",
            "  Batch 21,080  of  27,551.    Elapsed: 3:43:57.\n",
            "  Batch 21,120  of  27,551.    Elapsed: 3:44:22.\n",
            "  Batch 21,160  of  27,551.    Elapsed: 3:44:48.\n",
            "  Batch 21,200  of  27,551.    Elapsed: 3:45:13.\n",
            "  Batch 21,240  of  27,551.    Elapsed: 3:45:39.\n",
            "  Batch 21,280  of  27,551.    Elapsed: 3:46:04.\n",
            "  Batch 21,320  of  27,551.    Elapsed: 3:46:29.\n",
            "  Batch 21,360  of  27,551.    Elapsed: 3:46:55.\n",
            "  Batch 21,400  of  27,551.    Elapsed: 3:47:20.\n",
            "  Batch 21,440  of  27,551.    Elapsed: 3:47:46.\n",
            "  Batch 21,480  of  27,551.    Elapsed: 3:48:11.\n",
            "  Batch 21,520  of  27,551.    Elapsed: 3:48:37.\n",
            "  Batch 21,560  of  27,551.    Elapsed: 3:49:02.\n",
            "  Batch 21,600  of  27,551.    Elapsed: 3:49:28.\n",
            "  Batch 21,640  of  27,551.    Elapsed: 3:49:53.\n",
            "  Batch 21,680  of  27,551.    Elapsed: 3:50:19.\n",
            "  Batch 21,720  of  27,551.    Elapsed: 3:50:44.\n",
            "  Batch 21,760  of  27,551.    Elapsed: 3:51:10.\n",
            "  Batch 21,800  of  27,551.    Elapsed: 3:51:35.\n",
            "  Batch 21,840  of  27,551.    Elapsed: 3:52:01.\n",
            "  Batch 21,880  of  27,551.    Elapsed: 3:52:26.\n",
            "  Batch 21,920  of  27,551.    Elapsed: 3:52:52.\n",
            "  Batch 21,960  of  27,551.    Elapsed: 3:53:17.\n",
            "  Batch 22,000  of  27,551.    Elapsed: 3:53:43.\n",
            "  Batch 22,040  of  27,551.    Elapsed: 3:54:08.\n",
            "  Batch 22,080  of  27,551.    Elapsed: 3:54:34.\n",
            "  Batch 22,120  of  27,551.    Elapsed: 3:54:59.\n",
            "  Batch 22,160  of  27,551.    Elapsed: 3:55:25.\n",
            "  Batch 22,200  of  27,551.    Elapsed: 3:55:50.\n",
            "  Batch 22,240  of  27,551.    Elapsed: 3:56:16.\n",
            "  Batch 22,280  of  27,551.    Elapsed: 3:56:41.\n",
            "  Batch 22,320  of  27,551.    Elapsed: 3:57:07.\n",
            "  Batch 22,360  of  27,551.    Elapsed: 3:57:32.\n",
            "  Batch 22,400  of  27,551.    Elapsed: 3:57:58.\n",
            "  Batch 22,440  of  27,551.    Elapsed: 3:58:23.\n",
            "  Batch 22,480  of  27,551.    Elapsed: 3:58:49.\n",
            "  Batch 22,520  of  27,551.    Elapsed: 3:59:14.\n",
            "  Batch 22,560  of  27,551.    Elapsed: 3:59:40.\n",
            "  Batch 22,600  of  27,551.    Elapsed: 4:00:05.\n",
            "  Batch 22,640  of  27,551.    Elapsed: 4:00:31.\n",
            "  Batch 22,680  of  27,551.    Elapsed: 4:00:56.\n",
            "  Batch 22,720  of  27,551.    Elapsed: 4:01:22.\n",
            "  Batch 22,760  of  27,551.    Elapsed: 4:01:47.\n",
            "  Batch 22,800  of  27,551.    Elapsed: 4:02:13.\n",
            "  Batch 22,840  of  27,551.    Elapsed: 4:02:38.\n",
            "  Batch 22,880  of  27,551.    Elapsed: 4:03:04.\n",
            "  Batch 22,920  of  27,551.    Elapsed: 4:03:29.\n",
            "  Batch 22,960  of  27,551.    Elapsed: 4:03:55.\n",
            "  Batch 23,000  of  27,551.    Elapsed: 4:04:20.\n",
            "  Batch 23,040  of  27,551.    Elapsed: 4:04:46.\n",
            "  Batch 23,080  of  27,551.    Elapsed: 4:05:11.\n",
            "  Batch 23,120  of  27,551.    Elapsed: 4:05:37.\n",
            "  Batch 23,160  of  27,551.    Elapsed: 4:06:02.\n",
            "  Batch 23,200  of  27,551.    Elapsed: 4:06:28.\n",
            "  Batch 23,240  of  27,551.    Elapsed: 4:06:53.\n",
            "  Batch 23,280  of  27,551.    Elapsed: 4:07:19.\n",
            "  Batch 23,320  of  27,551.    Elapsed: 4:07:44.\n",
            "  Batch 23,360  of  27,551.    Elapsed: 4:08:10.\n",
            "  Batch 23,400  of  27,551.    Elapsed: 4:08:35.\n",
            "  Batch 23,440  of  27,551.    Elapsed: 4:09:01.\n",
            "  Batch 23,480  of  27,551.    Elapsed: 4:09:26.\n",
            "  Batch 23,520  of  27,551.    Elapsed: 4:09:52.\n",
            "  Batch 23,560  of  27,551.    Elapsed: 4:10:17.\n",
            "  Batch 23,600  of  27,551.    Elapsed: 4:10:43.\n",
            "  Batch 23,640  of  27,551.    Elapsed: 4:11:08.\n",
            "  Batch 23,680  of  27,551.    Elapsed: 4:11:34.\n",
            "  Batch 23,720  of  27,551.    Elapsed: 4:11:59.\n",
            "  Batch 23,760  of  27,551.    Elapsed: 4:12:25.\n",
            "  Batch 23,800  of  27,551.    Elapsed: 4:12:50.\n",
            "  Batch 23,840  of  27,551.    Elapsed: 4:13:16.\n",
            "  Batch 23,880  of  27,551.    Elapsed: 4:13:41.\n",
            "  Batch 23,920  of  27,551.    Elapsed: 4:14:07.\n",
            "  Batch 23,960  of  27,551.    Elapsed: 4:14:32.\n",
            "  Batch 24,000  of  27,551.    Elapsed: 4:14:58.\n",
            "  Batch 24,040  of  27,551.    Elapsed: 4:15:23.\n",
            "  Batch 24,080  of  27,551.    Elapsed: 4:15:49.\n",
            "  Batch 24,120  of  27,551.    Elapsed: 4:16:14.\n",
            "  Batch 24,160  of  27,551.    Elapsed: 4:16:40.\n",
            "  Batch 24,200  of  27,551.    Elapsed: 4:17:05.\n",
            "  Batch 24,240  of  27,551.    Elapsed: 4:17:31.\n",
            "  Batch 24,280  of  27,551.    Elapsed: 4:17:56.\n",
            "  Batch 24,320  of  27,551.    Elapsed: 4:18:22.\n",
            "  Batch 24,360  of  27,551.    Elapsed: 4:18:47.\n",
            "  Batch 24,400  of  27,551.    Elapsed: 4:19:13.\n",
            "  Batch 24,440  of  27,551.    Elapsed: 4:19:38.\n",
            "  Batch 24,480  of  27,551.    Elapsed: 4:20:04.\n",
            "  Batch 24,520  of  27,551.    Elapsed: 4:20:29.\n",
            "  Batch 24,560  of  27,551.    Elapsed: 4:20:55.\n",
            "  Batch 24,600  of  27,551.    Elapsed: 4:21:20.\n",
            "  Batch 24,640  of  27,551.    Elapsed: 4:21:46.\n",
            "  Batch 24,680  of  27,551.    Elapsed: 4:22:11.\n",
            "  Batch 24,720  of  27,551.    Elapsed: 4:22:37.\n",
            "  Batch 24,760  of  27,551.    Elapsed: 4:23:03.\n",
            "  Batch 24,800  of  27,551.    Elapsed: 4:23:28.\n",
            "  Batch 24,840  of  27,551.    Elapsed: 4:23:53.\n",
            "  Batch 24,880  of  27,551.    Elapsed: 4:24:19.\n",
            "  Batch 24,920  of  27,551.    Elapsed: 4:24:44.\n",
            "  Batch 24,960  of  27,551.    Elapsed: 4:25:10.\n",
            "  Batch 25,000  of  27,551.    Elapsed: 4:25:36.\n",
            "  Batch 25,040  of  27,551.    Elapsed: 4:26:01.\n",
            "  Batch 25,080  of  27,551.    Elapsed: 4:26:27.\n",
            "  Batch 25,120  of  27,551.    Elapsed: 4:26:52.\n",
            "  Batch 25,160  of  27,551.    Elapsed: 4:27:17.\n",
            "  Batch 25,200  of  27,551.    Elapsed: 4:27:43.\n",
            "  Batch 25,240  of  27,551.    Elapsed: 4:28:08.\n",
            "  Batch 25,280  of  27,551.    Elapsed: 4:28:34.\n",
            "  Batch 25,320  of  27,551.    Elapsed: 4:28:59.\n",
            "  Batch 25,360  of  27,551.    Elapsed: 4:29:25.\n",
            "  Batch 25,400  of  27,551.    Elapsed: 4:29:50.\n",
            "  Batch 25,440  of  27,551.    Elapsed: 4:30:16.\n",
            "  Batch 25,480  of  27,551.    Elapsed: 4:30:41.\n",
            "  Batch 25,520  of  27,551.    Elapsed: 4:31:07.\n",
            "  Batch 25,560  of  27,551.    Elapsed: 4:31:32.\n",
            "  Batch 25,600  of  27,551.    Elapsed: 4:31:58.\n",
            "  Batch 25,640  of  27,551.    Elapsed: 4:32:23.\n",
            "  Batch 25,680  of  27,551.    Elapsed: 4:32:49.\n",
            "  Batch 25,720  of  27,551.    Elapsed: 4:33:14.\n",
            "  Batch 25,760  of  27,551.    Elapsed: 4:33:40.\n",
            "  Batch 25,800  of  27,551.    Elapsed: 4:34:05.\n",
            "  Batch 25,840  of  27,551.    Elapsed: 4:34:31.\n",
            "  Batch 25,880  of  27,551.    Elapsed: 4:34:56.\n",
            "  Batch 25,920  of  27,551.    Elapsed: 4:35:22.\n",
            "  Batch 25,960  of  27,551.    Elapsed: 4:35:47.\n",
            "  Batch 26,000  of  27,551.    Elapsed: 4:36:13.\n",
            "  Batch 26,040  of  27,551.    Elapsed: 4:36:38.\n",
            "  Batch 26,080  of  27,551.    Elapsed: 4:37:04.\n",
            "  Batch 26,120  of  27,551.    Elapsed: 4:37:29.\n",
            "  Batch 26,160  of  27,551.    Elapsed: 4:37:55.\n",
            "  Batch 26,200  of  27,551.    Elapsed: 4:38:20.\n",
            "  Batch 26,240  of  27,551.    Elapsed: 4:38:46.\n",
            "  Batch 26,280  of  27,551.    Elapsed: 4:39:11.\n",
            "  Batch 26,320  of  27,551.    Elapsed: 4:39:37.\n",
            "  Batch 26,360  of  27,551.    Elapsed: 4:40:02.\n",
            "  Batch 26,400  of  27,551.    Elapsed: 4:40:28.\n",
            "  Batch 26,440  of  27,551.    Elapsed: 4:40:53.\n",
            "  Batch 26,480  of  27,551.    Elapsed: 4:41:19.\n",
            "  Batch 26,520  of  27,551.    Elapsed: 4:41:44.\n",
            "  Batch 26,560  of  27,551.    Elapsed: 4:42:10.\n",
            "  Batch 26,600  of  27,551.    Elapsed: 4:42:35.\n",
            "  Batch 26,640  of  27,551.    Elapsed: 4:43:01.\n",
            "  Batch 26,680  of  27,551.    Elapsed: 4:43:26.\n",
            "  Batch 26,720  of  27,551.    Elapsed: 4:43:52.\n",
            "  Batch 26,760  of  27,551.    Elapsed: 4:44:17.\n",
            "  Batch 26,800  of  27,551.    Elapsed: 4:44:43.\n",
            "  Batch 26,840  of  27,551.    Elapsed: 4:45:08.\n",
            "  Batch 26,880  of  27,551.    Elapsed: 4:45:34.\n",
            "  Batch 26,920  of  27,551.    Elapsed: 4:45:59.\n",
            "  Batch 26,960  of  27,551.    Elapsed: 4:46:25.\n",
            "  Batch 27,000  of  27,551.    Elapsed: 4:46:50.\n",
            "  Batch 27,040  of  27,551.    Elapsed: 4:47:16.\n",
            "  Batch 27,080  of  27,551.    Elapsed: 4:47:41.\n",
            "  Batch 27,120  of  27,551.    Elapsed: 4:48:07.\n",
            "  Batch 27,160  of  27,551.    Elapsed: 4:48:32.\n",
            "  Batch 27,200  of  27,551.    Elapsed: 4:48:58.\n",
            "  Batch 27,240  of  27,551.    Elapsed: 4:49:23.\n",
            "  Batch 27,280  of  27,551.    Elapsed: 4:49:49.\n",
            "  Batch 27,320  of  27,551.    Elapsed: 4:50:14.\n",
            "  Batch 27,360  of  27,551.    Elapsed: 4:50:40.\n",
            "  Batch 27,400  of  27,551.    Elapsed: 4:51:05.\n",
            "  Batch 27,440  of  27,551.    Elapsed: 4:51:31.\n",
            "  Batch 27,480  of  27,551.    Elapsed: 4:51:56.\n",
            "  Batch 27,520  of  27,551.    Elapsed: 4:52:22.\n",
            "\n",
            "  Average training loss: 0.11\n",
            "  Training epcoh took: 4:52:42\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.9610\n",
            "  Precision: 0.7315\n",
            "  Recall: 0.5847\n",
            "  F1 Score: 0.6499\n",
            "  Validation Loss: 0.0989\n",
            "  Validation took: 0:26:45\n",
            "Saving model to ./model_save/\n",
            "\n",
            "======== Epoch 2 / 2 ========\n",
            "Training...\n",
            "  Batch    40  of  27,551.    Elapsed: 0:00:26.\n",
            "  Batch    80  of  27,551.    Elapsed: 0:00:51.\n",
            "  Batch   120  of  27,551.    Elapsed: 0:01:17.\n",
            "  Batch   160  of  27,551.    Elapsed: 0:01:42.\n",
            "  Batch   200  of  27,551.    Elapsed: 0:02:08.\n",
            "  Batch   240  of  27,551.    Elapsed: 0:02:33.\n",
            "  Batch   280  of  27,551.    Elapsed: 0:02:59.\n",
            "  Batch   320  of  27,551.    Elapsed: 0:03:24.\n",
            "  Batch   360  of  27,551.    Elapsed: 0:03:49.\n",
            "  Batch   400  of  27,551.    Elapsed: 0:04:15.\n",
            "  Batch   440  of  27,551.    Elapsed: 0:04:40.\n",
            "  Batch   480  of  27,551.    Elapsed: 0:05:06.\n",
            "  Batch   520  of  27,551.    Elapsed: 0:05:31.\n",
            "  Batch   560  of  27,551.    Elapsed: 0:05:57.\n",
            "  Batch   600  of  27,551.    Elapsed: 0:06:22.\n",
            "  Batch   640  of  27,551.    Elapsed: 0:06:48.\n",
            "  Batch   680  of  27,551.    Elapsed: 0:07:13.\n",
            "  Batch   720  of  27,551.    Elapsed: 0:07:39.\n",
            "  Batch   760  of  27,551.    Elapsed: 0:08:04.\n",
            "  Batch   800  of  27,551.    Elapsed: 0:08:30.\n",
            "  Batch   840  of  27,551.    Elapsed: 0:08:55.\n",
            "  Batch   880  of  27,551.    Elapsed: 0:09:21.\n",
            "  Batch   920  of  27,551.    Elapsed: 0:09:46.\n",
            "  Batch   960  of  27,551.    Elapsed: 0:10:12.\n",
            "  Batch 1,000  of  27,551.    Elapsed: 0:10:37.\n",
            "  Batch 1,040  of  27,551.    Elapsed: 0:11:03.\n",
            "  Batch 1,080  of  27,551.    Elapsed: 0:11:28.\n",
            "  Batch 1,120  of  27,551.    Elapsed: 0:11:54.\n",
            "  Batch 1,160  of  27,551.    Elapsed: 0:12:19.\n",
            "  Batch 1,200  of  27,551.    Elapsed: 0:12:45.\n",
            "  Batch 1,240  of  27,551.    Elapsed: 0:13:10.\n",
            "  Batch 1,280  of  27,551.    Elapsed: 0:13:36.\n",
            "  Batch 1,320  of  27,551.    Elapsed: 0:14:01.\n",
            "  Batch 1,360  of  27,551.    Elapsed: 0:14:27.\n",
            "  Batch 1,400  of  27,551.    Elapsed: 0:14:52.\n",
            "  Batch 1,440  of  27,551.    Elapsed: 0:15:18.\n",
            "  Batch 1,480  of  27,551.    Elapsed: 0:15:43.\n",
            "  Batch 1,520  of  27,551.    Elapsed: 0:16:09.\n",
            "  Batch 1,560  of  27,551.    Elapsed: 0:16:34.\n",
            "  Batch 1,600  of  27,551.    Elapsed: 0:17:00.\n",
            "  Batch 1,640  of  27,551.    Elapsed: 0:17:25.\n",
            "  Batch 1,680  of  27,551.    Elapsed: 0:17:51.\n",
            "  Batch 1,720  of  27,551.    Elapsed: 0:18:16.\n",
            "  Batch 1,760  of  27,551.    Elapsed: 0:18:42.\n",
            "  Batch 1,800  of  27,551.    Elapsed: 0:19:07.\n",
            "  Batch 1,840  of  27,551.    Elapsed: 0:19:32.\n",
            "  Batch 1,880  of  27,551.    Elapsed: 0:19:58.\n",
            "  Batch 1,920  of  27,551.    Elapsed: 0:20:23.\n",
            "  Batch 1,960  of  27,551.    Elapsed: 0:20:49.\n",
            "  Batch 2,000  of  27,551.    Elapsed: 0:21:14.\n",
            "  Batch 2,040  of  27,551.    Elapsed: 0:21:40.\n",
            "  Batch 2,080  of  27,551.    Elapsed: 0:22:05.\n",
            "  Batch 2,120  of  27,551.    Elapsed: 0:22:31.\n",
            "  Batch 2,160  of  27,551.    Elapsed: 0:22:56.\n",
            "  Batch 2,200  of  27,551.    Elapsed: 0:23:22.\n",
            "  Batch 2,240  of  27,551.    Elapsed: 0:23:47.\n",
            "  Batch 2,280  of  27,551.    Elapsed: 0:24:13.\n",
            "  Batch 2,320  of  27,551.    Elapsed: 0:24:38.\n",
            "  Batch 2,360  of  27,551.    Elapsed: 0:25:04.\n",
            "  Batch 2,400  of  27,551.    Elapsed: 0:25:29.\n",
            "  Batch 2,440  of  27,551.    Elapsed: 0:25:55.\n",
            "  Batch 2,480  of  27,551.    Elapsed: 0:26:20.\n",
            "  Batch 2,520  of  27,551.    Elapsed: 0:26:46.\n",
            "  Batch 2,560  of  27,551.    Elapsed: 0:27:11.\n",
            "  Batch 2,600  of  27,551.    Elapsed: 0:27:37.\n",
            "  Batch 2,640  of  27,551.    Elapsed: 0:28:02.\n",
            "  Batch 2,680  of  27,551.    Elapsed: 0:28:28.\n",
            "  Batch 2,720  of  27,551.    Elapsed: 0:28:53.\n",
            "  Batch 2,760  of  27,551.    Elapsed: 0:29:19.\n",
            "  Batch 2,800  of  27,551.    Elapsed: 0:29:44.\n",
            "  Batch 2,840  of  27,551.    Elapsed: 0:30:10.\n",
            "  Batch 2,880  of  27,551.    Elapsed: 0:30:35.\n",
            "  Batch 2,920  of  27,551.    Elapsed: 0:31:01.\n",
            "  Batch 2,960  of  27,551.    Elapsed: 0:31:26.\n",
            "  Batch 3,000  of  27,551.    Elapsed: 0:31:52.\n",
            "  Batch 3,040  of  27,551.    Elapsed: 0:32:17.\n",
            "  Batch 3,080  of  27,551.    Elapsed: 0:32:43.\n",
            "  Batch 3,120  of  27,551.    Elapsed: 0:33:08.\n",
            "  Batch 3,160  of  27,551.    Elapsed: 0:33:34.\n",
            "  Batch 3,200  of  27,551.    Elapsed: 0:33:59.\n",
            "  Batch 3,240  of  27,551.    Elapsed: 0:34:24.\n",
            "  Batch 3,280  of  27,551.    Elapsed: 0:34:50.\n",
            "  Batch 3,320  of  27,551.    Elapsed: 0:35:15.\n",
            "  Batch 3,360  of  27,551.    Elapsed: 0:35:41.\n",
            "  Batch 3,400  of  27,551.    Elapsed: 0:36:06.\n",
            "  Batch 3,440  of  27,551.    Elapsed: 0:36:32.\n",
            "  Batch 3,480  of  27,551.    Elapsed: 0:36:57.\n",
            "  Batch 3,520  of  27,551.    Elapsed: 0:37:23.\n",
            "  Batch 3,560  of  27,551.    Elapsed: 0:37:48.\n",
            "  Batch 3,600  of  27,551.    Elapsed: 0:38:14.\n",
            "  Batch 3,640  of  27,551.    Elapsed: 0:38:39.\n",
            "  Batch 3,680  of  27,551.    Elapsed: 0:39:05.\n",
            "  Batch 3,720  of  27,551.    Elapsed: 0:39:30.\n",
            "  Batch 3,760  of  27,551.    Elapsed: 0:39:56.\n",
            "  Batch 3,800  of  27,551.    Elapsed: 0:40:21.\n",
            "  Batch 3,840  of  27,551.    Elapsed: 0:40:47.\n",
            "  Batch 3,880  of  27,551.    Elapsed: 0:41:12.\n",
            "  Batch 3,920  of  27,551.    Elapsed: 0:41:38.\n",
            "  Batch 3,960  of  27,551.    Elapsed: 0:42:03.\n",
            "  Batch 4,000  of  27,551.    Elapsed: 0:42:29.\n",
            "  Batch 4,040  of  27,551.    Elapsed: 0:42:54.\n",
            "  Batch 4,080  of  27,551.    Elapsed: 0:43:20.\n",
            "  Batch 4,120  of  27,551.    Elapsed: 0:43:45.\n",
            "  Batch 4,160  of  27,551.    Elapsed: 0:44:11.\n",
            "  Batch 4,200  of  27,551.    Elapsed: 0:44:36.\n",
            "  Batch 4,240  of  27,551.    Elapsed: 0:45:02.\n",
            "  Batch 4,280  of  27,551.    Elapsed: 0:45:27.\n",
            "  Batch 4,320  of  27,551.    Elapsed: 0:45:53.\n",
            "  Batch 4,360  of  27,551.    Elapsed: 0:46:18.\n",
            "  Batch 4,400  of  27,551.    Elapsed: 0:46:44.\n",
            "  Batch 4,440  of  27,551.    Elapsed: 0:47:09.\n",
            "  Batch 4,480  of  27,551.    Elapsed: 0:47:35.\n",
            "  Batch 4,520  of  27,551.    Elapsed: 0:48:00.\n",
            "  Batch 4,560  of  27,551.    Elapsed: 0:48:26.\n",
            "  Batch 4,600  of  27,551.    Elapsed: 0:48:51.\n",
            "  Batch 4,640  of  27,551.    Elapsed: 0:49:17.\n",
            "  Batch 4,680  of  27,551.    Elapsed: 0:49:42.\n",
            "  Batch 4,720  of  27,551.    Elapsed: 0:50:08.\n",
            "  Batch 4,760  of  27,551.    Elapsed: 0:50:33.\n",
            "  Batch 4,800  of  27,551.    Elapsed: 0:50:59.\n",
            "  Batch 4,840  of  27,551.    Elapsed: 0:51:24.\n",
            "  Batch 4,880  of  27,551.    Elapsed: 0:51:50.\n",
            "  Batch 4,920  of  27,551.    Elapsed: 0:52:15.\n",
            "  Batch 4,960  of  27,551.    Elapsed: 0:52:41.\n",
            "  Batch 5,000  of  27,551.    Elapsed: 0:53:06.\n",
            "  Batch 5,040  of  27,551.    Elapsed: 0:53:32.\n",
            "  Batch 5,080  of  27,551.    Elapsed: 0:53:57.\n",
            "  Batch 5,120  of  27,551.    Elapsed: 0:54:23.\n",
            "  Batch 5,160  of  27,551.    Elapsed: 0:54:48.\n",
            "  Batch 5,200  of  27,551.    Elapsed: 0:55:14.\n",
            "  Batch 5,240  of  27,551.    Elapsed: 0:55:39.\n",
            "  Batch 5,280  of  27,551.    Elapsed: 0:56:05.\n",
            "  Batch 5,320  of  27,551.    Elapsed: 0:56:30.\n",
            "  Batch 5,360  of  27,551.    Elapsed: 0:56:56.\n",
            "  Batch 5,400  of  27,551.    Elapsed: 0:57:21.\n",
            "  Batch 5,440  of  27,551.    Elapsed: 0:57:47.\n",
            "  Batch 5,480  of  27,551.    Elapsed: 0:58:12.\n",
            "  Batch 5,520  of  27,551.    Elapsed: 0:58:38.\n",
            "  Batch 5,560  of  27,551.    Elapsed: 0:59:03.\n",
            "  Batch 5,600  of  27,551.    Elapsed: 0:59:29.\n",
            "  Batch 5,640  of  27,551.    Elapsed: 0:59:54.\n",
            "  Batch 5,680  of  27,551.    Elapsed: 1:00:20.\n",
            "  Batch 5,720  of  27,551.    Elapsed: 1:00:45.\n",
            "  Batch 5,760  of  27,551.    Elapsed: 1:01:11.\n",
            "  Batch 5,800  of  27,551.    Elapsed: 1:01:36.\n",
            "  Batch 5,840  of  27,551.    Elapsed: 1:02:02.\n",
            "  Batch 5,880  of  27,551.    Elapsed: 1:02:27.\n",
            "  Batch 5,920  of  27,551.    Elapsed: 1:02:52.\n",
            "  Batch 5,960  of  27,551.    Elapsed: 1:03:18.\n",
            "  Batch 6,000  of  27,551.    Elapsed: 1:03:43.\n",
            "  Batch 6,040  of  27,551.    Elapsed: 1:04:09.\n",
            "  Batch 6,080  of  27,551.    Elapsed: 1:04:34.\n",
            "  Batch 6,120  of  27,551.    Elapsed: 1:05:00.\n",
            "  Batch 6,160  of  27,551.    Elapsed: 1:05:25.\n",
            "  Batch 6,200  of  27,551.    Elapsed: 1:05:51.\n",
            "  Batch 6,240  of  27,551.    Elapsed: 1:06:16.\n",
            "  Batch 6,280  of  27,551.    Elapsed: 1:06:42.\n",
            "  Batch 6,320  of  27,551.    Elapsed: 1:07:07.\n",
            "  Batch 6,360  of  27,551.    Elapsed: 1:07:33.\n",
            "  Batch 6,400  of  27,551.    Elapsed: 1:07:58.\n",
            "  Batch 6,440  of  27,551.    Elapsed: 1:08:24.\n",
            "  Batch 6,480  of  27,551.    Elapsed: 1:08:49.\n",
            "  Batch 6,520  of  27,551.    Elapsed: 1:09:15.\n",
            "  Batch 6,560  of  27,551.    Elapsed: 1:09:40.\n",
            "  Batch 6,600  of  27,551.    Elapsed: 1:10:06.\n",
            "  Batch 6,640  of  27,551.    Elapsed: 1:10:31.\n",
            "  Batch 6,680  of  27,551.    Elapsed: 1:10:57.\n",
            "  Batch 6,720  of  27,551.    Elapsed: 1:11:22.\n",
            "  Batch 6,760  of  27,551.    Elapsed: 1:11:48.\n",
            "  Batch 6,800  of  27,551.    Elapsed: 1:12:13.\n",
            "  Batch 6,840  of  27,551.    Elapsed: 1:12:39.\n",
            "  Batch 6,880  of  27,551.    Elapsed: 1:13:04.\n",
            "  Batch 6,920  of  27,551.    Elapsed: 1:13:30.\n",
            "  Batch 6,960  of  27,551.    Elapsed: 1:13:55.\n",
            "  Batch 7,000  of  27,551.    Elapsed: 1:14:21.\n",
            "  Batch 7,040  of  27,551.    Elapsed: 1:14:46.\n",
            "  Batch 7,080  of  27,551.    Elapsed: 1:15:12.\n",
            "  Batch 7,120  of  27,551.    Elapsed: 1:15:37.\n",
            "  Batch 7,160  of  27,551.    Elapsed: 1:16:03.\n",
            "  Batch 7,200  of  27,551.    Elapsed: 1:16:28.\n",
            "  Batch 7,240  of  27,551.    Elapsed: 1:16:54.\n",
            "  Batch 7,280  of  27,551.    Elapsed: 1:17:19.\n",
            "  Batch 7,320  of  27,551.    Elapsed: 1:17:45.\n",
            "  Batch 7,360  of  27,551.    Elapsed: 1:18:10.\n",
            "  Batch 7,400  of  27,551.    Elapsed: 1:18:36.\n",
            "  Batch 7,440  of  27,551.    Elapsed: 1:19:01.\n",
            "  Batch 7,480  of  27,551.    Elapsed: 1:19:27.\n",
            "  Batch 7,520  of  27,551.    Elapsed: 1:19:52.\n",
            "  Batch 7,560  of  27,551.    Elapsed: 1:20:18.\n",
            "  Batch 7,600  of  27,551.    Elapsed: 1:20:43.\n",
            "  Batch 7,640  of  27,551.    Elapsed: 1:21:09.\n",
            "  Batch 7,680  of  27,551.    Elapsed: 1:21:34.\n",
            "  Batch 7,720  of  27,551.    Elapsed: 1:22:00.\n",
            "  Batch 7,760  of  27,551.    Elapsed: 1:22:25.\n",
            "  Batch 7,800  of  27,551.    Elapsed: 1:22:51.\n",
            "  Batch 7,840  of  27,551.    Elapsed: 1:23:16.\n",
            "  Batch 7,880  of  27,551.    Elapsed: 1:23:42.\n",
            "  Batch 7,920  of  27,551.    Elapsed: 1:24:07.\n",
            "  Batch 7,960  of  27,551.    Elapsed: 1:24:33.\n",
            "  Batch 8,000  of  27,551.    Elapsed: 1:24:58.\n",
            "  Batch 8,040  of  27,551.    Elapsed: 1:25:24.\n",
            "  Batch 8,080  of  27,551.    Elapsed: 1:25:49.\n",
            "  Batch 8,120  of  27,551.    Elapsed: 1:26:15.\n",
            "  Batch 8,160  of  27,551.    Elapsed: 1:26:40.\n",
            "  Batch 8,200  of  27,551.    Elapsed: 1:27:06.\n",
            "  Batch 8,240  of  27,551.    Elapsed: 1:27:31.\n",
            "  Batch 8,280  of  27,551.    Elapsed: 1:27:57.\n",
            "  Batch 8,320  of  27,551.    Elapsed: 1:28:22.\n",
            "  Batch 8,360  of  27,551.    Elapsed: 1:28:48.\n",
            "  Batch 8,400  of  27,551.    Elapsed: 1:29:13.\n",
            "  Batch 8,440  of  27,551.    Elapsed: 1:29:39.\n",
            "  Batch 8,480  of  27,551.    Elapsed: 1:30:04.\n",
            "  Batch 8,520  of  27,551.    Elapsed: 1:30:30.\n",
            "  Batch 8,560  of  27,551.    Elapsed: 1:30:55.\n",
            "  Batch 8,600  of  27,551.    Elapsed: 1:31:21.\n",
            "  Batch 8,640  of  27,551.    Elapsed: 1:31:46.\n",
            "  Batch 8,680  of  27,551.    Elapsed: 1:32:12.\n",
            "  Batch 8,720  of  27,551.    Elapsed: 1:32:37.\n",
            "  Batch 8,760  of  27,551.    Elapsed: 1:33:03.\n",
            "  Batch 8,800  of  27,551.    Elapsed: 1:33:28.\n",
            "  Batch 8,840  of  27,551.    Elapsed: 1:33:54.\n",
            "  Batch 8,880  of  27,551.    Elapsed: 1:34:19.\n",
            "  Batch 8,920  of  27,551.    Elapsed: 1:34:45.\n",
            "  Batch 8,960  of  27,551.    Elapsed: 1:35:10.\n",
            "  Batch 9,000  of  27,551.    Elapsed: 1:35:36.\n",
            "  Batch 9,040  of  27,551.    Elapsed: 1:36:01.\n",
            "  Batch 9,080  of  27,551.    Elapsed: 1:36:27.\n",
            "  Batch 9,120  of  27,551.    Elapsed: 1:36:52.\n",
            "  Batch 9,160  of  27,551.    Elapsed: 1:37:18.\n",
            "  Batch 9,200  of  27,551.    Elapsed: 1:37:43.\n",
            "  Batch 9,240  of  27,551.    Elapsed: 1:38:09.\n",
            "  Batch 9,280  of  27,551.    Elapsed: 1:38:34.\n",
            "  Batch 9,320  of  27,551.    Elapsed: 1:39:00.\n",
            "  Batch 9,360  of  27,551.    Elapsed: 1:39:25.\n",
            "  Batch 9,400  of  27,551.    Elapsed: 1:39:51.\n",
            "  Batch 9,440  of  27,551.    Elapsed: 1:40:16.\n",
            "  Batch 9,480  of  27,551.    Elapsed: 1:40:41.\n",
            "  Batch 9,520  of  27,551.    Elapsed: 1:41:07.\n",
            "  Batch 9,560  of  27,551.    Elapsed: 1:41:32.\n",
            "  Batch 9,600  of  27,551.    Elapsed: 1:41:58.\n",
            "  Batch 9,640  of  27,551.    Elapsed: 1:42:23.\n",
            "  Batch 9,680  of  27,551.    Elapsed: 1:42:49.\n",
            "  Batch 9,720  of  27,551.    Elapsed: 1:43:14.\n",
            "  Batch 9,760  of  27,551.    Elapsed: 1:43:40.\n",
            "  Batch 9,800  of  27,551.    Elapsed: 1:44:05.\n",
            "  Batch 9,840  of  27,551.    Elapsed: 1:44:31.\n",
            "  Batch 9,880  of  27,551.    Elapsed: 1:44:56.\n",
            "  Batch 9,920  of  27,551.    Elapsed: 1:45:22.\n",
            "  Batch 9,960  of  27,551.    Elapsed: 1:45:47.\n",
            "  Batch 10,000  of  27,551.    Elapsed: 1:46:13.\n",
            "  Batch 10,040  of  27,551.    Elapsed: 1:46:38.\n",
            "  Batch 10,080  of  27,551.    Elapsed: 1:47:04.\n",
            "  Batch 10,120  of  27,551.    Elapsed: 1:47:29.\n",
            "  Batch 10,160  of  27,551.    Elapsed: 1:47:55.\n",
            "  Batch 10,200  of  27,551.    Elapsed: 1:48:20.\n",
            "  Batch 10,240  of  27,551.    Elapsed: 1:48:46.\n",
            "  Batch 10,280  of  27,551.    Elapsed: 1:49:11.\n",
            "  Batch 10,320  of  27,551.    Elapsed: 1:49:37.\n",
            "  Batch 10,360  of  27,551.    Elapsed: 1:50:02.\n",
            "  Batch 10,400  of  27,551.    Elapsed: 1:50:28.\n",
            "  Batch 10,440  of  27,551.    Elapsed: 1:50:53.\n",
            "  Batch 10,480  of  27,551.    Elapsed: 1:51:19.\n",
            "  Batch 10,520  of  27,551.    Elapsed: 1:51:44.\n",
            "  Batch 10,560  of  27,551.    Elapsed: 1:52:10.\n",
            "  Batch 10,600  of  27,551.    Elapsed: 1:52:35.\n",
            "  Batch 10,640  of  27,551.    Elapsed: 1:53:01.\n",
            "  Batch 10,680  of  27,551.    Elapsed: 1:53:26.\n",
            "  Batch 10,720  of  27,551.    Elapsed: 1:53:52.\n",
            "  Batch 10,760  of  27,551.    Elapsed: 1:54:17.\n",
            "  Batch 10,800  of  27,551.    Elapsed: 1:54:43.\n",
            "  Batch 10,840  of  27,551.    Elapsed: 1:55:08.\n",
            "  Batch 10,880  of  27,551.    Elapsed: 1:55:34.\n",
            "  Batch 10,920  of  27,551.    Elapsed: 1:55:59.\n",
            "  Batch 10,960  of  27,551.    Elapsed: 1:56:25.\n",
            "  Batch 11,000  of  27,551.    Elapsed: 1:56:50.\n",
            "  Batch 11,040  of  27,551.    Elapsed: 1:57:16.\n",
            "  Batch 11,080  of  27,551.    Elapsed: 1:57:41.\n",
            "  Batch 11,120  of  27,551.    Elapsed: 1:58:07.\n",
            "  Batch 11,160  of  27,551.    Elapsed: 1:58:32.\n",
            "  Batch 11,200  of  27,551.    Elapsed: 1:58:58.\n",
            "  Batch 11,240  of  27,551.    Elapsed: 1:59:23.\n",
            "  Batch 11,280  of  27,551.    Elapsed: 1:59:49.\n",
            "  Batch 11,320  of  27,551.    Elapsed: 2:00:14.\n",
            "  Batch 11,360  of  27,551.    Elapsed: 2:00:40.\n",
            "  Batch 11,400  of  27,551.    Elapsed: 2:01:05.\n",
            "  Batch 11,440  of  27,551.    Elapsed: 2:01:31.\n",
            "  Batch 11,480  of  27,551.    Elapsed: 2:01:56.\n",
            "  Batch 11,520  of  27,551.    Elapsed: 2:02:22.\n",
            "  Batch 11,560  of  27,551.    Elapsed: 2:02:47.\n",
            "  Batch 11,600  of  27,551.    Elapsed: 2:03:13.\n",
            "  Batch 11,640  of  27,551.    Elapsed: 2:03:38.\n",
            "  Batch 11,680  of  27,551.    Elapsed: 2:04:04.\n",
            "  Batch 11,720  of  27,551.    Elapsed: 2:04:29.\n",
            "  Batch 11,760  of  27,551.    Elapsed: 2:04:54.\n",
            "  Batch 11,800  of  27,551.    Elapsed: 2:05:20.\n",
            "  Batch 11,840  of  27,551.    Elapsed: 2:05:45.\n",
            "  Batch 11,880  of  27,551.    Elapsed: 2:06:11.\n",
            "  Batch 11,920  of  27,551.    Elapsed: 2:06:36.\n",
            "  Batch 11,960  of  27,551.    Elapsed: 2:07:02.\n",
            "  Batch 12,000  of  27,551.    Elapsed: 2:07:27.\n",
            "  Batch 12,040  of  27,551.    Elapsed: 2:07:53.\n",
            "  Batch 12,080  of  27,551.    Elapsed: 2:08:18.\n",
            "  Batch 12,120  of  27,551.    Elapsed: 2:08:44.\n",
            "  Batch 12,160  of  27,551.    Elapsed: 2:09:09.\n",
            "  Batch 12,200  of  27,551.    Elapsed: 2:09:35.\n",
            "  Batch 12,240  of  27,551.    Elapsed: 2:10:00.\n",
            "  Batch 12,280  of  27,551.    Elapsed: 2:10:26.\n",
            "  Batch 12,320  of  27,551.    Elapsed: 2:10:51.\n",
            "  Batch 12,360  of  27,551.    Elapsed: 2:11:17.\n",
            "  Batch 12,400  of  27,551.    Elapsed: 2:11:42.\n",
            "  Batch 12,440  of  27,551.    Elapsed: 2:12:08.\n",
            "  Batch 12,480  of  27,551.    Elapsed: 2:12:33.\n",
            "  Batch 12,520  of  27,551.    Elapsed: 2:12:59.\n",
            "  Batch 12,560  of  27,551.    Elapsed: 2:13:24.\n",
            "  Batch 12,600  of  27,551.    Elapsed: 2:13:50.\n",
            "  Batch 12,640  of  27,551.    Elapsed: 2:14:15.\n",
            "  Batch 12,680  of  27,551.    Elapsed: 2:14:41.\n",
            "  Batch 12,720  of  27,551.    Elapsed: 2:15:06.\n",
            "  Batch 12,760  of  27,551.    Elapsed: 2:15:32.\n",
            "  Batch 12,800  of  27,551.    Elapsed: 2:15:57.\n",
            "  Batch 12,840  of  27,551.    Elapsed: 2:16:23.\n",
            "  Batch 12,880  of  27,551.    Elapsed: 2:16:48.\n",
            "  Batch 12,920  of  27,551.    Elapsed: 2:17:14.\n",
            "  Batch 12,960  of  27,551.    Elapsed: 2:17:39.\n",
            "  Batch 13,000  of  27,551.    Elapsed: 2:18:05.\n",
            "  Batch 13,040  of  27,551.    Elapsed: 2:18:30.\n",
            "  Batch 13,080  of  27,551.    Elapsed: 2:18:56.\n",
            "  Batch 13,120  of  27,551.    Elapsed: 2:19:21.\n",
            "  Batch 13,160  of  27,551.    Elapsed: 2:19:47.\n",
            "  Batch 13,200  of  27,551.    Elapsed: 2:20:12.\n",
            "  Batch 13,240  of  27,551.    Elapsed: 2:20:38.\n",
            "  Batch 13,280  of  27,551.    Elapsed: 2:21:03.\n",
            "  Batch 13,320  of  27,551.    Elapsed: 2:21:29.\n",
            "  Batch 13,360  of  27,551.    Elapsed: 2:21:54.\n",
            "  Batch 13,400  of  27,551.    Elapsed: 2:22:20.\n",
            "  Batch 13,440  of  27,551.    Elapsed: 2:22:45.\n",
            "  Batch 13,480  of  27,551.    Elapsed: 2:23:11.\n",
            "  Batch 13,520  of  27,551.    Elapsed: 2:23:36.\n",
            "  Batch 13,560  of  27,551.    Elapsed: 2:24:02.\n",
            "  Batch 13,600  of  27,551.    Elapsed: 2:24:27.\n",
            "  Batch 13,640  of  27,551.    Elapsed: 2:24:53.\n",
            "  Batch 13,680  of  27,551.    Elapsed: 2:25:18.\n",
            "  Batch 13,720  of  27,551.    Elapsed: 2:25:44.\n",
            "  Batch 13,760  of  27,551.    Elapsed: 2:26:09.\n",
            "  Batch 13,800  of  27,551.    Elapsed: 2:26:35.\n",
            "  Batch 13,840  of  27,551.    Elapsed: 2:27:00.\n",
            "  Batch 13,880  of  27,551.    Elapsed: 2:27:26.\n",
            "  Batch 13,920  of  27,551.    Elapsed: 2:27:51.\n",
            "  Batch 13,960  of  27,551.    Elapsed: 2:28:17.\n",
            "  Batch 14,000  of  27,551.    Elapsed: 2:28:42.\n",
            "  Batch 14,040  of  27,551.    Elapsed: 2:29:08.\n",
            "  Batch 14,080  of  27,551.    Elapsed: 2:29:33.\n",
            "  Batch 14,120  of  27,551.    Elapsed: 2:29:59.\n",
            "  Batch 14,160  of  27,551.    Elapsed: 2:30:24.\n",
            "  Batch 14,200  of  27,551.    Elapsed: 2:30:50.\n",
            "  Batch 14,240  of  27,551.    Elapsed: 2:31:15.\n",
            "  Batch 14,280  of  27,551.    Elapsed: 2:31:41.\n",
            "  Batch 14,320  of  27,551.    Elapsed: 2:32:06.\n",
            "  Batch 14,360  of  27,551.    Elapsed: 2:32:32.\n",
            "  Batch 14,400  of  27,551.    Elapsed: 2:32:57.\n",
            "  Batch 14,440  of  27,551.    Elapsed: 2:33:23.\n",
            "  Batch 14,480  of  27,551.    Elapsed: 2:33:48.\n",
            "  Batch 14,520  of  27,551.    Elapsed: 2:34:14.\n",
            "  Batch 14,560  of  27,551.    Elapsed: 2:34:39.\n",
            "  Batch 14,600  of  27,551.    Elapsed: 2:35:05.\n",
            "  Batch 14,640  of  27,551.    Elapsed: 2:35:30.\n",
            "  Batch 14,680  of  27,551.    Elapsed: 2:35:56.\n",
            "  Batch 14,720  of  27,551.    Elapsed: 2:36:21.\n",
            "  Batch 14,760  of  27,551.    Elapsed: 2:36:47.\n",
            "  Batch 14,800  of  27,551.    Elapsed: 2:37:12.\n",
            "  Batch 14,840  of  27,551.    Elapsed: 2:37:37.\n",
            "  Batch 14,880  of  27,551.    Elapsed: 2:38:03.\n",
            "  Batch 14,920  of  27,551.    Elapsed: 2:38:28.\n",
            "  Batch 14,960  of  27,551.    Elapsed: 2:38:54.\n",
            "  Batch 15,000  of  27,551.    Elapsed: 2:39:19.\n",
            "  Batch 15,040  of  27,551.    Elapsed: 2:39:45.\n",
            "  Batch 15,080  of  27,551.    Elapsed: 2:40:10.\n",
            "  Batch 15,120  of  27,551.    Elapsed: 2:40:36.\n",
            "  Batch 15,160  of  27,551.    Elapsed: 2:41:01.\n",
            "  Batch 15,200  of  27,551.    Elapsed: 2:41:27.\n",
            "  Batch 15,240  of  27,551.    Elapsed: 2:41:52.\n",
            "  Batch 15,280  of  27,551.    Elapsed: 2:42:18.\n",
            "  Batch 15,320  of  27,551.    Elapsed: 2:42:43.\n",
            "  Batch 15,360  of  27,551.    Elapsed: 2:43:09.\n",
            "  Batch 15,400  of  27,551.    Elapsed: 2:43:34.\n",
            "  Batch 15,440  of  27,551.    Elapsed: 2:44:00.\n",
            "  Batch 15,480  of  27,551.    Elapsed: 2:44:25.\n",
            "  Batch 15,520  of  27,551.    Elapsed: 2:44:51.\n",
            "  Batch 15,560  of  27,551.    Elapsed: 2:45:16.\n",
            "  Batch 15,600  of  27,551.    Elapsed: 2:45:42.\n",
            "  Batch 15,640  of  27,551.    Elapsed: 2:46:07.\n",
            "  Batch 15,680  of  27,551.    Elapsed: 2:46:33.\n",
            "  Batch 15,720  of  27,551.    Elapsed: 2:46:58.\n",
            "  Batch 15,760  of  27,551.    Elapsed: 2:47:24.\n",
            "  Batch 15,800  of  27,551.    Elapsed: 2:47:49.\n",
            "  Batch 15,840  of  27,551.    Elapsed: 2:48:15.\n",
            "  Batch 15,880  of  27,551.    Elapsed: 2:48:40.\n",
            "  Batch 15,920  of  27,551.    Elapsed: 2:49:06.\n",
            "  Batch 15,960  of  27,551.    Elapsed: 2:49:31.\n",
            "  Batch 16,000  of  27,551.    Elapsed: 2:49:57.\n",
            "  Batch 16,040  of  27,551.    Elapsed: 2:50:22.\n",
            "  Batch 16,080  of  27,551.    Elapsed: 2:50:48.\n",
            "  Batch 16,120  of  27,551.    Elapsed: 2:51:13.\n",
            "  Batch 16,160  of  27,551.    Elapsed: 2:51:39.\n",
            "  Batch 16,200  of  27,551.    Elapsed: 2:52:04.\n",
            "  Batch 16,240  of  27,551.    Elapsed: 2:52:30.\n",
            "  Batch 16,280  of  27,551.    Elapsed: 2:52:55.\n",
            "  Batch 16,320  of  27,551.    Elapsed: 2:53:21.\n",
            "  Batch 16,360  of  27,551.    Elapsed: 2:53:46.\n",
            "  Batch 16,400  of  27,551.    Elapsed: 2:54:12.\n",
            "  Batch 16,440  of  27,551.    Elapsed: 2:54:37.\n",
            "  Batch 16,480  of  27,551.    Elapsed: 2:55:03.\n",
            "  Batch 16,520  of  27,551.    Elapsed: 2:55:28.\n",
            "  Batch 16,560  of  27,551.    Elapsed: 2:55:54.\n",
            "  Batch 16,600  of  27,551.    Elapsed: 2:56:19.\n",
            "  Batch 16,640  of  27,551.    Elapsed: 2:56:45.\n",
            "  Batch 16,680  of  27,551.    Elapsed: 2:57:10.\n",
            "  Batch 16,720  of  27,551.    Elapsed: 2:57:36.\n",
            "  Batch 16,760  of  27,551.    Elapsed: 2:58:01.\n",
            "  Batch 16,800  of  27,551.    Elapsed: 2:58:27.\n",
            "  Batch 16,840  of  27,551.    Elapsed: 2:58:52.\n",
            "  Batch 16,880  of  27,551.    Elapsed: 2:59:18.\n",
            "  Batch 16,920  of  27,551.    Elapsed: 2:59:43.\n",
            "  Batch 16,960  of  27,551.    Elapsed: 3:00:09.\n",
            "  Batch 17,000  of  27,551.    Elapsed: 3:00:34.\n",
            "  Batch 17,040  of  27,551.    Elapsed: 3:01:00.\n",
            "  Batch 17,080  of  27,551.    Elapsed: 3:01:25.\n",
            "  Batch 17,120  of  27,551.    Elapsed: 3:01:51.\n",
            "  Batch 17,160  of  27,551.    Elapsed: 3:02:16.\n",
            "  Batch 17,200  of  27,551.    Elapsed: 3:02:42.\n",
            "  Batch 17,240  of  27,551.    Elapsed: 3:03:07.\n",
            "  Batch 17,280  of  27,551.    Elapsed: 3:03:33.\n",
            "  Batch 17,320  of  27,551.    Elapsed: 3:03:58.\n",
            "  Batch 17,360  of  27,551.    Elapsed: 3:04:24.\n",
            "  Batch 17,400  of  27,551.    Elapsed: 3:04:49.\n",
            "  Batch 17,440  of  27,551.    Elapsed: 3:05:15.\n",
            "  Batch 17,480  of  27,551.    Elapsed: 3:05:40.\n",
            "  Batch 17,520  of  27,551.    Elapsed: 3:06:06.\n",
            "  Batch 17,560  of  27,551.    Elapsed: 3:06:31.\n",
            "  Batch 17,600  of  27,551.    Elapsed: 3:06:57.\n",
            "  Batch 17,640  of  27,551.    Elapsed: 3:07:22.\n",
            "  Batch 17,680  of  27,551.    Elapsed: 3:07:48.\n",
            "  Batch 17,720  of  27,551.    Elapsed: 3:08:13.\n",
            "  Batch 17,760  of  27,551.    Elapsed: 3:08:39.\n",
            "  Batch 17,800  of  27,551.    Elapsed: 3:09:04.\n",
            "  Batch 17,840  of  27,551.    Elapsed: 3:09:30.\n",
            "  Batch 17,880  of  27,551.    Elapsed: 3:09:55.\n",
            "  Batch 17,920  of  27,551.    Elapsed: 3:10:21.\n",
            "  Batch 17,960  of  27,551.    Elapsed: 3:10:46.\n",
            "  Batch 18,000  of  27,551.    Elapsed: 3:11:12.\n",
            "  Batch 18,040  of  27,551.    Elapsed: 3:11:37.\n",
            "  Batch 18,080  of  27,551.    Elapsed: 3:12:02.\n",
            "  Batch 18,120  of  27,551.    Elapsed: 3:12:28.\n",
            "  Batch 18,160  of  27,551.    Elapsed: 3:12:53.\n",
            "  Batch 18,200  of  27,551.    Elapsed: 3:13:19.\n",
            "  Batch 18,240  of  27,551.    Elapsed: 3:13:44.\n",
            "  Batch 18,280  of  27,551.    Elapsed: 3:14:10.\n",
            "  Batch 18,320  of  27,551.    Elapsed: 3:14:35.\n",
            "  Batch 18,360  of  27,551.    Elapsed: 3:15:01.\n",
            "  Batch 18,400  of  27,551.    Elapsed: 3:15:26.\n",
            "  Batch 18,440  of  27,551.    Elapsed: 3:15:52.\n",
            "  Batch 18,480  of  27,551.    Elapsed: 3:16:17.\n",
            "  Batch 18,520  of  27,551.    Elapsed: 3:16:43.\n",
            "  Batch 18,560  of  27,551.    Elapsed: 3:17:08.\n",
            "  Batch 18,600  of  27,551.    Elapsed: 3:17:34.\n",
            "  Batch 18,640  of  27,551.    Elapsed: 3:17:59.\n",
            "  Batch 18,680  of  27,551.    Elapsed: 3:18:25.\n",
            "  Batch 18,720  of  27,551.    Elapsed: 3:18:50.\n",
            "  Batch 18,760  of  27,551.    Elapsed: 3:19:16.\n",
            "  Batch 18,800  of  27,551.    Elapsed: 3:19:41.\n",
            "  Batch 18,840  of  27,551.    Elapsed: 3:20:07.\n",
            "  Batch 18,880  of  27,551.    Elapsed: 3:20:32.\n",
            "  Batch 18,920  of  27,551.    Elapsed: 3:20:58.\n",
            "  Batch 18,960  of  27,551.    Elapsed: 3:21:23.\n",
            "  Batch 19,000  of  27,551.    Elapsed: 3:21:49.\n",
            "  Batch 19,040  of  27,551.    Elapsed: 3:22:14.\n",
            "  Batch 19,080  of  27,551.    Elapsed: 3:22:40.\n",
            "  Batch 19,120  of  27,551.    Elapsed: 3:23:05.\n",
            "  Batch 19,160  of  27,551.    Elapsed: 3:23:31.\n",
            "  Batch 19,200  of  27,551.    Elapsed: 3:23:56.\n",
            "  Batch 19,240  of  27,551.    Elapsed: 3:24:22.\n",
            "  Batch 19,280  of  27,551.    Elapsed: 3:24:47.\n",
            "  Batch 19,320  of  27,551.    Elapsed: 3:25:13.\n",
            "  Batch 19,360  of  27,551.    Elapsed: 3:25:38.\n",
            "  Batch 19,400  of  27,551.    Elapsed: 3:26:04.\n",
            "  Batch 19,440  of  27,551.    Elapsed: 3:26:29.\n",
            "  Batch 19,480  of  27,551.    Elapsed: 3:26:55.\n",
            "  Batch 19,520  of  27,551.    Elapsed: 3:27:20.\n",
            "  Batch 19,560  of  27,551.    Elapsed: 3:27:46.\n",
            "  Batch 19,600  of  27,551.    Elapsed: 3:28:11.\n",
            "  Batch 19,640  of  27,551.    Elapsed: 3:28:37.\n",
            "  Batch 19,680  of  27,551.    Elapsed: 3:29:02.\n",
            "  Batch 19,720  of  27,551.    Elapsed: 3:29:28.\n",
            "  Batch 19,760  of  27,551.    Elapsed: 3:29:53.\n",
            "  Batch 19,800  of  27,551.    Elapsed: 3:30:19.\n",
            "  Batch 19,840  of  27,551.    Elapsed: 3:30:44.\n",
            "  Batch 19,880  of  27,551.    Elapsed: 3:31:10.\n",
            "  Batch 19,920  of  27,551.    Elapsed: 3:31:35.\n",
            "  Batch 19,960  of  27,551.    Elapsed: 3:32:01.\n",
            "  Batch 20,000  of  27,551.    Elapsed: 3:32:26.\n",
            "  Batch 20,040  of  27,551.    Elapsed: 3:32:52.\n",
            "  Batch 20,080  of  27,551.    Elapsed: 3:33:17.\n",
            "  Batch 20,120  of  27,551.    Elapsed: 3:33:43.\n",
            "  Batch 20,160  of  27,551.    Elapsed: 3:34:08.\n",
            "  Batch 20,200  of  27,551.    Elapsed: 3:34:34.\n",
            "  Batch 20,240  of  27,551.    Elapsed: 3:34:59.\n",
            "  Batch 20,280  of  27,551.    Elapsed: 3:35:25.\n",
            "  Batch 20,320  of  27,551.    Elapsed: 3:35:50.\n",
            "  Batch 20,360  of  27,551.    Elapsed: 3:36:16.\n",
            "  Batch 20,400  of  27,551.    Elapsed: 3:36:41.\n",
            "  Batch 20,440  of  27,551.    Elapsed: 3:37:07.\n",
            "  Batch 20,480  of  27,551.    Elapsed: 3:37:32.\n",
            "  Batch 20,520  of  27,551.    Elapsed: 3:37:58.\n",
            "  Batch 20,560  of  27,551.    Elapsed: 3:38:23.\n",
            "  Batch 20,600  of  27,551.    Elapsed: 3:38:49.\n",
            "  Batch 20,640  of  27,551.    Elapsed: 3:39:14.\n",
            "  Batch 20,680  of  27,551.    Elapsed: 3:39:40.\n",
            "  Batch 20,720  of  27,551.    Elapsed: 3:40:05.\n",
            "  Batch 20,760  of  27,551.    Elapsed: 3:40:31.\n",
            "  Batch 20,800  of  27,551.    Elapsed: 3:40:56.\n",
            "  Batch 20,840  of  27,551.    Elapsed: 3:41:22.\n",
            "  Batch 20,880  of  27,551.    Elapsed: 3:41:47.\n",
            "  Batch 20,920  of  27,551.    Elapsed: 3:42:13.\n",
            "  Batch 20,960  of  27,551.    Elapsed: 3:42:38.\n",
            "  Batch 21,000  of  27,551.    Elapsed: 3:43:04.\n",
            "  Batch 21,040  of  27,551.    Elapsed: 3:43:29.\n",
            "  Batch 21,080  of  27,551.    Elapsed: 3:43:55.\n",
            "  Batch 21,120  of  27,551.    Elapsed: 3:44:20.\n",
            "  Batch 21,160  of  27,551.    Elapsed: 3:44:46.\n",
            "  Batch 21,200  of  27,551.    Elapsed: 3:45:11.\n",
            "  Batch 21,240  of  27,551.    Elapsed: 3:45:37.\n",
            "  Batch 21,280  of  27,551.    Elapsed: 3:46:02.\n",
            "  Batch 21,320  of  27,551.    Elapsed: 3:46:28.\n",
            "  Batch 21,360  of  27,551.    Elapsed: 3:46:53.\n",
            "  Batch 21,400  of  27,551.    Elapsed: 3:47:19.\n",
            "  Batch 21,440  of  27,551.    Elapsed: 3:47:44.\n",
            "  Batch 21,480  of  27,551.    Elapsed: 3:48:10.\n",
            "  Batch 21,520  of  27,551.    Elapsed: 3:48:35.\n",
            "  Batch 21,560  of  27,551.    Elapsed: 3:49:01.\n",
            "  Batch 21,600  of  27,551.    Elapsed: 3:49:26.\n",
            "  Batch 21,640  of  27,551.    Elapsed: 3:49:52.\n",
            "  Batch 21,680  of  27,551.    Elapsed: 3:50:17.\n",
            "  Batch 21,720  of  27,551.    Elapsed: 3:50:43.\n",
            "  Batch 21,760  of  27,551.    Elapsed: 3:51:08.\n",
            "  Batch 21,800  of  27,551.    Elapsed: 3:51:34.\n",
            "  Batch 21,840  of  27,551.    Elapsed: 3:51:59.\n",
            "  Batch 21,880  of  27,551.    Elapsed: 3:52:25.\n",
            "  Batch 21,920  of  27,551.    Elapsed: 3:52:50.\n",
            "  Batch 21,960  of  27,551.    Elapsed: 3:53:16.\n",
            "  Batch 22,000  of  27,551.    Elapsed: 3:53:41.\n",
            "  Batch 22,040  of  27,551.    Elapsed: 3:54:07.\n",
            "  Batch 22,080  of  27,551.    Elapsed: 3:54:32.\n",
            "  Batch 22,120  of  27,551.    Elapsed: 3:54:57.\n",
            "  Batch 22,160  of  27,551.    Elapsed: 3:55:23.\n",
            "  Batch 22,200  of  27,551.    Elapsed: 3:55:48.\n",
            "  Batch 22,240  of  27,551.    Elapsed: 3:56:14.\n",
            "  Batch 22,280  of  27,551.    Elapsed: 3:56:39.\n",
            "  Batch 22,320  of  27,551.    Elapsed: 3:57:05.\n",
            "  Batch 22,360  of  27,551.    Elapsed: 3:57:30.\n",
            "  Batch 22,400  of  27,551.    Elapsed: 3:57:56.\n",
            "  Batch 22,440  of  27,551.    Elapsed: 3:58:21.\n",
            "  Batch 22,480  of  27,551.    Elapsed: 3:58:47.\n",
            "  Batch 22,520  of  27,551.    Elapsed: 3:59:12.\n",
            "  Batch 22,560  of  27,551.    Elapsed: 3:59:38.\n",
            "  Batch 22,600  of  27,551.    Elapsed: 4:00:03.\n",
            "  Batch 22,640  of  27,551.    Elapsed: 4:00:29.\n",
            "  Batch 22,680  of  27,551.    Elapsed: 4:00:54.\n",
            "  Batch 22,720  of  27,551.    Elapsed: 4:01:20.\n",
            "  Batch 22,760  of  27,551.    Elapsed: 4:01:45.\n",
            "  Batch 22,800  of  27,551.    Elapsed: 4:02:11.\n",
            "  Batch 22,840  of  27,551.    Elapsed: 4:02:36.\n",
            "  Batch 22,880  of  27,551.    Elapsed: 4:03:02.\n",
            "  Batch 22,920  of  27,551.    Elapsed: 4:03:27.\n",
            "  Batch 22,960  of  27,551.    Elapsed: 4:03:53.\n",
            "  Batch 23,000  of  27,551.    Elapsed: 4:04:18.\n",
            "  Batch 23,040  of  27,551.    Elapsed: 4:04:44.\n",
            "  Batch 23,080  of  27,551.    Elapsed: 4:05:09.\n",
            "  Batch 23,120  of  27,551.    Elapsed: 4:05:35.\n",
            "  Batch 23,160  of  27,551.    Elapsed: 4:06:00.\n",
            "  Batch 23,200  of  27,551.    Elapsed: 4:06:26.\n",
            "  Batch 23,240  of  27,551.    Elapsed: 4:06:51.\n",
            "  Batch 23,280  of  27,551.    Elapsed: 4:07:16.\n",
            "  Batch 23,320  of  27,551.    Elapsed: 4:07:42.\n",
            "  Batch 23,360  of  27,551.    Elapsed: 4:08:07.\n",
            "  Batch 23,400  of  27,551.    Elapsed: 4:08:33.\n",
            "  Batch 23,440  of  27,551.    Elapsed: 4:08:58.\n",
            "  Batch 23,480  of  27,551.    Elapsed: 4:09:24.\n",
            "  Batch 23,520  of  27,551.    Elapsed: 4:09:49.\n",
            "  Batch 23,560  of  27,551.    Elapsed: 4:10:15.\n",
            "  Batch 23,600  of  27,551.    Elapsed: 4:10:40.\n",
            "  Batch 23,640  of  27,551.    Elapsed: 4:11:06.\n",
            "  Batch 23,680  of  27,551.    Elapsed: 4:11:31.\n",
            "  Batch 23,720  of  27,551.    Elapsed: 4:11:57.\n",
            "  Batch 23,760  of  27,551.    Elapsed: 4:12:22.\n",
            "  Batch 23,800  of  27,551.    Elapsed: 4:12:48.\n",
            "  Batch 23,840  of  27,551.    Elapsed: 4:13:13.\n",
            "  Batch 23,880  of  27,551.    Elapsed: 4:13:39.\n",
            "  Batch 23,920  of  27,551.    Elapsed: 4:14:04.\n",
            "  Batch 23,960  of  27,551.    Elapsed: 4:14:30.\n",
            "  Batch 24,000  of  27,551.    Elapsed: 4:14:55.\n",
            "  Batch 24,040  of  27,551.    Elapsed: 4:15:21.\n",
            "  Batch 24,080  of  27,551.    Elapsed: 4:15:46.\n",
            "  Batch 24,120  of  27,551.    Elapsed: 4:16:12.\n",
            "  Batch 24,160  of  27,551.    Elapsed: 4:16:37.\n",
            "  Batch 24,200  of  27,551.    Elapsed: 4:17:03.\n",
            "  Batch 24,240  of  27,551.    Elapsed: 4:17:28.\n",
            "  Batch 24,280  of  27,551.    Elapsed: 4:17:54.\n",
            "  Batch 24,320  of  27,551.    Elapsed: 4:18:19.\n",
            "  Batch 24,360  of  27,551.    Elapsed: 4:18:45.\n",
            "  Batch 24,400  of  27,551.    Elapsed: 4:19:10.\n",
            "  Batch 24,440  of  27,551.    Elapsed: 4:19:36.\n",
            "  Batch 24,480  of  27,551.    Elapsed: 4:20:01.\n",
            "  Batch 24,520  of  27,551.    Elapsed: 4:20:27.\n",
            "  Batch 24,560  of  27,551.    Elapsed: 4:20:52.\n",
            "  Batch 24,600  of  27,551.    Elapsed: 4:21:18.\n",
            "  Batch 24,640  of  27,551.    Elapsed: 4:21:43.\n",
            "  Batch 24,680  of  27,551.    Elapsed: 4:22:09.\n",
            "  Batch 24,720  of  27,551.    Elapsed: 4:22:34.\n",
            "  Batch 24,760  of  27,551.    Elapsed: 4:23:00.\n",
            "  Batch 24,800  of  27,551.    Elapsed: 4:23:25.\n",
            "  Batch 24,840  of  27,551.    Elapsed: 4:23:51.\n",
            "  Batch 24,880  of  27,551.    Elapsed: 4:24:16.\n",
            "  Batch 24,920  of  27,551.    Elapsed: 4:24:42.\n",
            "  Batch 24,960  of  27,551.    Elapsed: 4:25:07.\n",
            "  Batch 25,000  of  27,551.    Elapsed: 4:25:33.\n",
            "  Batch 25,040  of  27,551.    Elapsed: 4:25:58.\n",
            "  Batch 25,080  of  27,551.    Elapsed: 4:26:24.\n",
            "  Batch 25,120  of  27,551.    Elapsed: 4:26:49.\n",
            "  Batch 25,160  of  27,551.    Elapsed: 4:27:15.\n",
            "  Batch 25,200  of  27,551.    Elapsed: 4:27:40.\n",
            "  Batch 25,240  of  27,551.    Elapsed: 4:28:06.\n",
            "  Batch 25,280  of  27,551.    Elapsed: 4:28:31.\n",
            "  Batch 25,320  of  27,551.    Elapsed: 4:28:57.\n",
            "  Batch 25,360  of  27,551.    Elapsed: 4:29:22.\n",
            "  Batch 25,400  of  27,551.    Elapsed: 4:29:48.\n",
            "  Batch 25,440  of  27,551.    Elapsed: 4:30:13.\n",
            "  Batch 25,480  of  27,551.    Elapsed: 4:30:39.\n",
            "  Batch 25,520  of  27,551.    Elapsed: 4:31:04.\n",
            "  Batch 25,560  of  27,551.    Elapsed: 4:31:30.\n",
            "  Batch 25,600  of  27,551.    Elapsed: 4:31:55.\n",
            "  Batch 25,640  of  27,551.    Elapsed: 4:32:21.\n",
            "  Batch 25,680  of  27,551.    Elapsed: 4:32:46.\n",
            "  Batch 25,720  of  27,551.    Elapsed: 4:33:12.\n",
            "  Batch 25,760  of  27,551.    Elapsed: 4:33:37.\n",
            "  Batch 25,800  of  27,551.    Elapsed: 4:34:03.\n",
            "  Batch 25,840  of  27,551.    Elapsed: 4:34:28.\n",
            "  Batch 25,880  of  27,551.    Elapsed: 4:34:54.\n",
            "  Batch 25,920  of  27,551.    Elapsed: 4:35:19.\n",
            "  Batch 25,960  of  27,551.    Elapsed: 4:35:45.\n",
            "  Batch 26,000  of  27,551.    Elapsed: 4:36:10.\n",
            "  Batch 26,040  of  27,551.    Elapsed: 4:36:36.\n",
            "  Batch 26,080  of  27,551.    Elapsed: 4:37:01.\n",
            "  Batch 26,120  of  27,551.    Elapsed: 4:37:27.\n",
            "  Batch 26,160  of  27,551.    Elapsed: 4:37:52.\n",
            "  Batch 26,200  of  27,551.    Elapsed: 4:38:17.\n",
            "  Batch 26,240  of  27,551.    Elapsed: 4:38:43.\n",
            "  Batch 26,280  of  27,551.    Elapsed: 4:39:08.\n",
            "  Batch 26,320  of  27,551.    Elapsed: 4:39:34.\n",
            "  Batch 26,360  of  27,551.    Elapsed: 4:39:59.\n",
            "  Batch 26,400  of  27,551.    Elapsed: 4:40:25.\n",
            "  Batch 26,440  of  27,551.    Elapsed: 4:40:50.\n",
            "  Batch 26,480  of  27,551.    Elapsed: 4:41:16.\n",
            "  Batch 26,520  of  27,551.    Elapsed: 4:41:41.\n",
            "  Batch 26,560  of  27,551.    Elapsed: 4:42:07.\n",
            "  Batch 26,600  of  27,551.    Elapsed: 4:42:32.\n",
            "  Batch 26,640  of  27,551.    Elapsed: 4:42:58.\n",
            "  Batch 26,680  of  27,551.    Elapsed: 4:43:23.\n",
            "  Batch 26,720  of  27,551.    Elapsed: 4:43:49.\n",
            "  Batch 26,760  of  27,551.    Elapsed: 4:44:14.\n",
            "  Batch 26,800  of  27,551.    Elapsed: 4:44:40.\n",
            "  Batch 26,840  of  27,551.    Elapsed: 4:45:05.\n",
            "  Batch 26,880  of  27,551.    Elapsed: 4:45:31.\n",
            "  Batch 26,920  of  27,551.    Elapsed: 4:45:56.\n",
            "  Batch 26,960  of  27,551.    Elapsed: 4:46:22.\n",
            "  Batch 27,000  of  27,551.    Elapsed: 4:46:47.\n",
            "  Batch 27,040  of  27,551.    Elapsed: 4:47:13.\n",
            "  Batch 27,080  of  27,551.    Elapsed: 4:47:38.\n",
            "  Batch 27,120  of  27,551.    Elapsed: 4:48:04.\n",
            "  Batch 27,160  of  27,551.    Elapsed: 4:48:29.\n",
            "  Batch 27,200  of  27,551.    Elapsed: 4:48:55.\n",
            "  Batch 27,240  of  27,551.    Elapsed: 4:49:20.\n",
            "  Batch 27,280  of  27,551.    Elapsed: 4:49:46.\n",
            "  Batch 27,320  of  27,551.    Elapsed: 4:50:11.\n",
            "  Batch 27,360  of  27,551.    Elapsed: 4:50:37.\n",
            "  Batch 27,400  of  27,551.    Elapsed: 4:51:02.\n",
            "  Batch 27,440  of  27,551.    Elapsed: 4:51:28.\n",
            "  Batch 27,480  of  27,551.    Elapsed: 4:51:53.\n",
            "  Batch 27,520  of  27,551.    Elapsed: 4:52:19.\n",
            "\n",
            "  Average training loss: 0.10\n",
            "  Training epcoh took: 4:52:38\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.9619\n",
            "  Precision: 0.7314\n",
            "  Recall: 0.6068\n",
            "  F1 Score: 0.6633\n",
            "  Validation Loss: 0.0981\n",
            "  Validation took: 0:26:44\n",
            "Saving model to ./model_save/\n",
            "\n",
            "Training complete!\n",
            "Total training took 10:38:50 (h:mm:ss)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01gU-8jtDV0K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Copy the model files to a directory in your Google Drive.\n",
        "!cp -r ./model_save/ \"./drive/My Drive/BERT/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rahWb7TYi43h",
        "colab_type": "text"
      },
      "source": [
        "# Test on test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6K39w6cDeIl",
        "colab_type": "code",
        "outputId": "ea96c061-93d6-4916-8c5c-b8ed3b044cd1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# # Load a trained model and vocabulary that you have fine-tuned\n",
        "# model = model_class.from_pretrained(output_dir)\n",
        "#tokenizer = tokenizer_class.from_pretrained(output_dir)\n",
        "\n",
        "# # Copy the model to the GPU.\n",
        "model.to(device)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6AK6h8dXRiA",
        "colab_type": "code",
        "outputId": "4c523cbb-6b4f-44d3-f371-4a2f5eecc092",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "# Read the raw dataset\n",
        "df = pd.read_csv('val_split.csv')\n",
        "\n",
        "# Report the number of sentences.\n",
        "print('Number of test sentences: {:,}\\n'.format(df.shape[0]))\n",
        "\n",
        "# Report the number of sentences.\n",
        "print('Number of test sentences: {:,}\\n'.format(df.shape[0]))\n",
        "\n",
        "# Create sentence and label lists\n",
        "#sentences = df.sentence.values\n",
        "sentences=df.question_text.values\n",
        "labels = df.target.values\n",
        "\n",
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "\n",
        "# For every sentence...\n",
        "for sent in sentences:\n",
        "    # `encode_plus` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    #   (5) Pad or truncate the sentence to `max_length`\n",
        "    #   (6) Create attention masks for [PAD] tokens.\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                        max_length = 512,           # Pad & truncate all sentences.\n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,   # Construct attn. masks.\n",
        "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                   )\n",
        "    \n",
        "    # Add the encoded sentence to the list.  \n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "    \n",
        "    # And its attention mask (simply differentiates padding from non-padding).\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "# Convert the lists into tensors.\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "labels = torch.tensor(labels)\n",
        "\n",
        "# Set the batch size.  \n",
        "batch_size = 32  \n",
        "\n",
        "# Create the DataLoader.\n",
        "prediction_data = TensorDataset(input_ids, attention_masks, labels)\n",
        "prediction_sampler = SequentialSampler(prediction_data)\n",
        "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of test sentences: 326,531\n",
            "\n",
            "Number of test sentences: 326,531\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qvFyc3XiY42p",
        "colab_type": "code",
        "outputId": "cba3a249-4e0a-44a1-d4eb-3d0893f449fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        }
      },
      "source": [
        "# Prediction on test set\n",
        "\n",
        "print('Predicting labels for {:,} test sentences...'.format(len(input_ids)))\n",
        "\n",
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Tracking variables \n",
        "total_tn = 0\n",
        "total_tp = 0\n",
        "total_fp = 0\n",
        "total_fn = 0\n",
        "epsilon = 1e-7 \n",
        "\n",
        "# Predict \n",
        "for batch in prediction_dataloader:\n",
        "    # Add batch to GPU\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    \n",
        "    # Unpack the inputs from our dataloader\n",
        "    b_input_ids, b_input_mask, b_labels = batch\n",
        "    \n",
        "    # Telling the model not to compute or store gradients, saving memory and \n",
        "    # speeding up prediction\n",
        "    with torch.no_grad():\n",
        "        # Forward pass, calculate logit predictions\n",
        "        outputs = model(b_input_ids, token_type_ids=None, \n",
        "                        attention_mask=b_input_mask)\n",
        "\n",
        "    logits = outputs[0]\n",
        "\n",
        "    # Move logits and labels to CPU\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "    \n",
        "    # Store predictions and true labels\n",
        "    tn, fp, fn, tp = get_metrics(logits, label_ids)\n",
        "    total_tn += tn\n",
        "    total_tp += tp\n",
        "    total_fp += fp\n",
        "    total_fn += fn\n",
        "\n",
        "precision = total_tp / (total_tp + total_fp + epsilon)\n",
        "print(\"  Precision: {0:.4f}\".format(precision))\n",
        "\n",
        "recall = total_tp / (total_tp + total_fn + epsilon)\n",
        "print(\"  Recall: {0:.4f}\".format(recall))\n",
        "\n",
        "f1 = 2* (precision*recall) / (precision + recall + epsilon)\n",
        "print(\"  F1 Score: {0:.4f}\".format(f1))\n",
        "print('    DONE.')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicting labels for 326,531 test sentences...\n",
            "  Precision: 0.7272\n",
            "  Recall: 0.6176\n",
            "  F1 Score: 0.6679\n",
            "    DONE.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJ9dq9S2PACF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "np.save(\"./drive/My Drive/BERT/precision\", precision)\n",
        "np.save(\"./drive/My Drive/BERT/recall\", recall)\n",
        "np.save(\"./drive/My Drive/BERT/f1\", f1)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}